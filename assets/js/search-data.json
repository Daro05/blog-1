{
  
    
        "post0": {
            "title": "Citations in Fastpages via BibTeX and jekyll-scholar",
            "content": "(N.B.: The following was created as a Markdown file in _posts/. For Jupyter notebooks, the same things apply; one simply enters the Liquid codes into Markdown cells. Quick Jupyter example.) . How to Cite . For demonstration purposes, I’ll take the liberty of citing a couple of my recent papers, namely the first SignalTrain paper[1] and the new one by Billy Mitchell[2]. Instead of using the LaTeX code cite{ &lt;whatever&gt; }, I use the Liquid code {% cite &lt;whatever&gt; %}. For example, the first citation above is written as “{% cite signaltrain %}” in the Markdown file that generates this HTML page. . The two citation markings above point to the References section at the end of this post where the full references are printed out in the bibliography style of my choice. . Drawing from the Bibliography . In the main blog directory, create a new directory called _bibliography/, and place your BibTeX file there as references.bib. In the case of this demo, the references file looks like this: . @conference{signaltrain, title = {Profiling Audio Compressors with Deep Neural Networks}, author = {Hawley, Scott H. and Colburn, Benjamin and Mimilakis, Stylianos Ioannis}, booktitle = {Audio Engineering Society Convention 147}, month = {Oct}, year = {2019}, url = {http://www.aes.org/e-lib/browse.cfm?elib=20595} } @article{billy_signaltrain2, title={Exploring Quality and Generalizability in Parameterized Neural Audio Effects}, author={William Mitchell and Scott H. Hawley}, journal={ArXiv}, year={2020}, volume={abs/2006.05584} url = {https://arxiv.org/abs/2006.05584} } . Note that this (single) references file is for your entire blog. The great thing about this is that all your Jupyter notebooks and Markdown posts will draw from this same file, which could be hundreds of references long, and jekyll-scholar will only include the ones you need for each post. . Finally, at the end of your post, you signal the creation of the list of references by using the Liquid tag . {% bibliography --cited %} . …so I’ll put that at the very bottom of this file. (Currently that’ll generate an error, because we haven’t enabled jekyll-scholar yet, but we’ll do that next.) The optional argument --cited means it’ll only list the references cited in your post. . Enabling Jekyll-Scholar . To enable jekyll-scholar, all we need to do is make the following two changes, and perhaps a third. . In _config.yml, add “ - jekyll-scholar” to the list of plugins:. . | Edit the Gemfile to include gem &#39;jekyll-scholar&#39; where the other plugins are listed. . | Optional: The default citation format is “apa”. If you want to change that, you can add the following to your _config.yml file: . scholar: style: &lt;name&gt; . …naming one of the styles in the CSL style repository (but leaving off the .csl ending). Tip from the CSL maintainers: . To quickly search the styles in the GitHub CSL style repository by file name, press “t” to activate GitHub’s File Finder and start typing. . Note however that the csl-styles Gem package used by jekyll-scholar lags behind the official CSL style repository, so some names you choose may not work. In that case, you can supply a CSL file yourself. For this demo, I found the file physical-review-d.csl, added it to my main blog directory, and then specified the style name physical-review-d in _config.yml. This produced the bracketed-number citation markers above, and the reference format you see below in the References section. . | The convenience of this BibTeX/jekyll-scholar approach is that instead of having to manually edit references on each individual page – say, if you wanted to change citation formats (or alternatively, update information about a paper cited in multiple posts) – now you only change one line in _config.yml (or update one spot in references.bib) and the system “builds out” the change “everywhere.” . Happy blogging! . References . [1] S. H. Hawley, B. Colburn, &amp; S. I. Mimilakis, &quot;Profiling Audio Compressors with Deep Neural Networks,&quot; Audio Engineering Society Convention 147 (2019). . [2] W. Mitchell &amp; S. H. Hawley, &quot;Exploring Quality and Generalizability in Parameterized Neural Audio Effects,&quot; ArXiv abs/2006.05584 (2020). .",
            "url": "https://drscotthawley.github.io/blog/2020/07/01/Citations-Via-Bibtex.html",
            "relUrl": "/2020/07/01/Citations-Via-Bibtex.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
  
    
        ,"post2": {
            "title": "Data Are Measurements, Not the Things Themselves",
            "content": "Intro . There&#39;s a tendency in the data science to conflate data about people and things with the genuine articles. Furthermore, in the [online world], there is concern about being able to trust what we see online, as if the internet were an extension of our body&#39;s sensory apparatus. . My post today is a bit of &quot;ontology,&quot; but it has some practical consequences. . It&#39;s important to realize that data points often represent real people, and that the predictions and decisions of models can have real impact. It&#39;s also important to keep in mind that these data were obtained via various forms of measurements, which can involve reductions and biases. . This applies even when not dealing with people; it happens when working data of physical systems. One hopes that one&#39;s instruments accurately reflect object reality, but we know this is usually not quite the case. But since people are &#39;messy&#39; and highly variable, I&#39;d like to illustrate via an area less personal: Astronomy. . Take Astronomy, for Instance . As a graduate student studying astrophysics, I was shocked at some of the statistics work I saw being performed. I recall seeing a much more senior scientist show a circular-ball-shaped assortment of data points, to which he fit various lines, and proceeded to (try to) draw important physical inferences from these fits: . #collapse-hide import altair as alt import pandas as pd import numpy as np from numpy.random import normal # Generate some random data np.random.seed(2) N = 500 x = normal(loc=1.0, size=N) y = normal(loc=1.0, size=N) source = pd.DataFrame({&quot;x&quot;: x, &quot;y&quot;: y}) # Define the degree of the polynomial fits degree_list = [2] base = alt.Chart(source).mark_circle(color=&quot;black&quot;).encode( alt.X(&quot;x&quot;), alt.Y(&quot;y&quot;) ) polynomial_fit = [ base.transform_regression( &quot;x&quot;, &quot;y&quot;, method=&quot;poly&quot;, order=order, as_=[&quot;x&quot;, str(order)] ) .mark_line() .transform_fold([str(order)], as_=[&quot;degree&quot;, &quot;y&quot;]) #.encode(alt.Color(&quot;degree:N&quot;)) for order in degree_list ] alt.layer(base, *polynomial_fit) . . Figure 1: Example of a graph I once saw an established astronomer try to fit lines to. . All I could say to this, as a young intern, was, &quot;....And that&#39;s a good fit?&quot; When I said that in front of this scientist and his boss, the boss laughed. . On another occasion, an enormously famous astronomer was giving a &quot;lunch talk&quot; and showed a graph like... . #collapse-hide np.random.seed(11) N = 25 x = normal(loc=0.25, scale=0.1, size=N) y = normal(loc=0.5, scale=0.2, size=N) x2 = normal(loc=0.5, scale=0.25, size=N) y2 = normal(loc=0.25, scale=0.1, size=N) x, y = np.concatenate((x,x2)), np.concatenate((y,y2)) source = pd.DataFrame({&quot;x&quot;: x, &quot;y&quot;: y}) base = alt.Chart(source).mark_point(clip=True).encode( alt.X(&quot;x&quot;, scale=alt.Scale(domain=(0, 1.5))), alt.Y(&quot;y&quot;, scale=alt.Scale(domain=(0, 1.2))) ) base . . Figure 2: Smattering of 25 data points representing stars ...and proceeded to say, &quot;Clearly we have two [different] populations of objects...&quot; . The Malmquist bias .",
            "url": "https://drscotthawley.github.io/blog/philosophy/data/deepfakes/2020/03/01/Data-Are-Measurements.html",
            "relUrl": "/philosophy/data/deepfakes/2020/03/01/Data-Are-Measurements.html",
            "date": " • Mar 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://drscotthawley.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://drscotthawley.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Principal Component Analysis (PCA) from Scratch",
            "content": ". Relevance . Principal Component Analysis (PCA) is a data-reduction technique that finds application in a wide variety of fields, including biology, sociology, physics, medicine, and audio processing. PCA may be used as a &quot;front end&quot; processing step that feeds into additional layers of machine learning, or it may be used by itself, for example when doing data visualization. It is so useful and ubiquitious that is is worth learning not only what it is for and what it is, but how to actually do it. . In this interactive worksheet, we work through how to perform PCA on a few different datasets, writing our own code as we go. . Other (better?) treatments . My treatment here was written several months after viewing... . the excellent demo page at setosa.io | this quick 1m30s video of a teapot, | this great StatsQuest video | this lecture from Andrew Ng&#39;s course | . Basic Idea . Put simply, PCA involves making a coordinate transformation (i.e., a rotation) from the arbitrary axes (or &quot;features&quot;) you started with to a set of axes &#39;aligned with the data itself,&#39; and doing this almost always means that you can get rid of a few of these &#39;components&#39; of data that have small variance without suffering much in the way of accurcy while saving yourself a ton of computation. . Once you &quot;get it,&quot; you&#39;ll find PCA to be almost no big deal, if it weren&#39;t for the fact that it&#39;s so darn useful! . We&#39;ll define the following terms as we go, but here&#39;s the process in a nutshell: . Covariance: Find the covariance matrix for your dataset | Eigenvectors: Find the eigenvectors of that matrix (these are the &quot;components&quot; btw) | Ordering: Sort the eigenvectors/&#39;dimensions&#39; from biggest to smallest variance | Projection / Data reduction: Use the eigenvectors corresponding to the largest variance to project the dataset into a reduced- dimensional space | (Check: How much did we lose by that truncation?) | Caveats . Since PCA will involve making linear transformations, there are some situations where PCA won&#39;t help but...pretty much it&#39;s handy enough that it&#39;s worth giving it a shot! . Covariance . If you&#39;ve got two data dimensions and they vary together, then they are co-variant. . Example: Two-dimensional data that&#39;s somewhat co-linear: . import numpy as np import matplotlib.pyplot as plt import plotly.graph_objects as go N = 100 x = np.random.normal(size=N) y = 0.5*x + 0.2*(np.random.normal(size=N)) fig = go.Figure(data=[go.Scatter(x=x, y=y, mode=&#39;markers&#39;, marker=dict(size=8,opacity=0.5), name=&quot;data&quot; )]) fig.update_layout( xaxis_title=&quot;x&quot;, yaxis_title=&quot;y&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . . . Variance . So, for just the $x$ component of the above data, there&#39;s some mean value (which in this case is zero), and there&#39;s some variance about this mean: technically the variance is the average of the squared differences from the mean. If you&#39;re familiar with the standard deviation, usually denoted by $ sigma$, the variance is just the square of the standard deviation. If $x$ had units of meters, the variance $ sigma^2$ would have units of meters^2. . Think of the variance as the &quot;spread,&quot; or &quot;extent&quot; of the data, about some particular axis (or input, or &quot;feature&quot;). . Similarly we can look at the variance in just the $y$ component of the data. For the above data, the variances in $x$ and $y$ are . print(&quot;Variance in x =&quot;,np.var(x)) print(&quot;Variance in y =&quot;,np.var(y)) . Variance in x = 0.6470431671825421 Variance in y = 0.19318628312072175 . Covariance . You&#39;ll notice in the above graph that as $x$ varies, so does $y$ -- pretty much. So $y$ is &quot;covariant&quot; with $x$. &quot;Covariance indicates the level to which two variables vary together.&quot; To compute it, it&#39;s kind of like the regular variance, except that instead of squaring the deviation from the mean for one variable, we multiply the deviations for the two variables: . $${ rm Cov}(x,y) = {1 over N-1} sum_{j=1}^N (x_j- mu_x)(y_j- mu_j),$$ where $ mu_x$ and $ mu_y$ are the means for the x- and y- componenets of the data, respectively. Note that you can reverse $x$ and $y$ and get the same result, and the covariance of a variable with itself is just the regular variance -- but with one caveat! . The caveat is that we&#39;re dividing by $N-1$ instead of $N$, so unlike the regular variance we&#39;re not quite taking the mean. Why this? Well, for large datasets this makes essentially no difference, but for small numbers of data points, using $N$ can give values that tend to be a bit too small for most people&#39;s tastes, so the $N-1$ was introduced to &quot;reduce small sample bias.&quot; . In Python code, the covariance calculation looks like . def covariance(a,b): return ( (a - a.mean())*(b - b.mean()) ).sum() / (len(a)-1) print(&quot;Covariance of x &amp; y =&quot;,covariance(x,y)) print(&quot;Covariance of y &amp; x =&quot;,covariance(x,y)) print(&quot;Covariance of x with itself =&quot;,covariance(x,x),&quot;, variance of x =&quot;,np.var(x)) print(&quot;Covariance of y with itself =&quot;,covariance(y,y),&quot;, variance of x =&quot;,np.var(y)) . Covariance of x &amp; y = 0.3211758726837525 Covariance of y &amp; x = 0.3211758726837525 Covariance of x with itself = 0.6535789567500425 , variance of x = 0.6470431671825421 Covariance of y with itself = 0.19513765971790076 , variance of x = 0.19318628312072175 . Covariance matrix . So what we do is we take the covariance of every variable with every variable (including itself) and make a matrix out of it. Along the diagonal will be the variance of each variable (except for that $N-1$ in the denominator), and the rest of the matrix will be the covariances. Note that since the order of the variables doesn&#39;t matter when computing covariance, the matrix will be symmetric (i.e. it will equal its own transpose, i.e. will have a reflection symmetry across the diagonal) and thus will be a square matrix. . Numpy gives us a handy thing to call: . data = np.stack((x,y),axis=1) # pack the x &amp; y data together in one 2D array print(&quot;data.shape =&quot;,data.shape) cov = np.cov(data.T) # .T b/c numpy wants varibles along rows rather than down columns? print(&quot;covariance matrix = n&quot;,cov) . data.shape = (100, 2) covariance matrix = [[0.65357896 0.32117587] [0.32117587 0.19513766]] . Some 3D data to work with . Now that we know what a covariance matrix is, let&#39;s generate some 3D data that we can use for what&#39;s coming next. Since there are 3 variables or 3 dimensions, the covariance matrix will now be 3x3. . z = -.5*x + 2*np.random.uniform(size=N) data = np.stack((x,y,z)).T print(&quot;data.shape =&quot;,data.shape) cov = np.cov(data.T) print(&quot;covariance matrix = n&quot;,cov) # Plot our data import plotly.graph_objects as go fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z,mode=&#39;markers&#39;, marker=dict(size=8,opacity=0.5), name=&quot;data&quot; )]) fig.update_layout( xaxis_title=&quot;x&quot;, yaxis_title=&quot;y&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . data.shape = (100, 3) covariance matrix = [[ 0.65357896 0.32117587 -0.33982198] [ 0.32117587 0.19513766 -0.15839307] [-0.33982198 -0.15839307 0.5207214 ]] . . . (Note that even though our $z$ data didn&#39;t explicitly depend on $y$, the fact that $y$ is covariant with $x$ means that $y$ and $z$ &#39;coincidentally&#39; have a nonzero covariance. This sort of thing shows up in many datasets where two variables are correlated and may give rise to &#39;confounding&#39; factors.) . So now we have a covariance matrix. The next thing in PCA is find the &#39;principal components&#39;. This means the directions along which the data varies the most. You can kind of estimate these by rotating the 3D graph above. See also this great YouTube video of a teapot (1min 30s) that explains PCA in this manner. . To do Principal Component Analysis, we need to find the aforementioned &quot;components,&quot; and this requires finding eigenvectors for our dataset&#39;s covariance matrix. . What is an eigenvector, really? . First a definition. (Stay with me! We&#39;ll flesh this out in what comes after this.) . Given some matrix (or &#39;linear operator&#39;) ${ bf A}$ with dimensions $n times n$ (i.e., $n$ rows and $n$ columns), there exist a set of $n$ vectors $ vec{v}_i$ (each with dimension $n$, and $i = 1...n$ counts which vector we&#39;re talking about) such that multiplying one of these vectors by ${ bf A}$ results in a vector (anti)parallel to $ vec{v}_i$, with a length that&#39;s multiplied by some constant $ lambda_i$. In equation form: . $${ bf A} vec{v}_i = lambda_i vec{v}_i, (1)$$ . where the constants $ lambda_i$ are called eigenvalues and the vectors $ vec{v}_i$ are called eigenvectors. . (Note that I&#39;m departing a bit from common notation that uses $ vec{x}_i$ instead of $ vec{v}_i$; I don&#39;t want people to get confused when I want to use $x$&#39;s for coordinate variables.) . A graphical version of this is shown in Figure 1: . . Figure 1. &quot;An eigenvector is a vector that a linear operator sends to a multiple of itself&quot; -- Daniel McLaury . Brief Q &amp; A before we go on: . &quot;So what&#39;s with the &#39;eigen&#39; part?&quot; That&#39;s a German prefix, which in this context means &quot;inherent&quot; or &quot;own&quot;. | &quot;Can a non-square matrix have eigenvectors?&quot; Well,...no, and think of it this way: If $ bf{A}$ were an $n times m$ matrix (where $m &lt;&gt; n$), then it would be mapping from $n$ dimensions into $m$ dimensions, but on the &quot;other side&quot; of the equation with the $ lambda_i vec{v}_i$, that would still have $n$ dimensions, so... you&#39;d be saying an $n$-dimensional object equals an $m$-dimensional object, which is a no-go. | &quot;But my dataset has many more rows than columns, so what am I supposed to do about that?&quot; Just wait! It&#39;ll be ok. We&#39;re not actually going to take the eigenvectors of the dataset &#39;directly&#39;, we&#39;re going to take the eigenvectors of the covariance matrix of the dataset. | &quot;Are eigenvectors important?&quot; You bet! They get used in many areas of science. I first encountered them in quantum mechanics.* They describe the &quot;principal vectors&quot; of many objects, or &quot;normal modes&quot; of oscillating systems. They get used in computer vision, and... lots of places. You&#39;ll see them almost anywhere matrices &amp; tensors are employed, such as our topic for today: Data science! | *Ok that&#39;s not quite true: I first encountered them in an extremely boring Linear Algebra class taught by an extremely boring NASA engineer who thought he wanted to try teaching. But it wasn&#39;t until later that I learned anything about their relevance for...anything. Consquently I didn&#39;t &quot;learn them&quot; very well so writing this is a helpful review for me. . How to find the eigenvectors of a matrix . You call a library routine that does it for you, of course! ;-) . from numpy import linalg as LA lambdas, vs = LA.eig(cov) lambdas, vs . (array([1.07311435, 0.26724004, 0.02908363]), array([[-0.73933506, -0.47534042, -0.47690162], [-0.3717427 , -0.30238807, 0.87770657], [ 0.56141877, -0.82620393, -0.04686177]])) . Ok sort of kidding; we&#39;ll do it &quot;from scratch&quot;. But, one caveat before we start: Some matrices can be &quot;weird&quot; or &quot;problematic&quot; and have things like &quot;singular values.&quot; There are sophisticated numerical libraries for doing this, and joking aside, for real-world numerical applications you&#39;re better off calling a library routine that other very smart and very careful people have written for you. But for now, we&#39;ll do the straightforward way which works pretty well for many cases. We&#39;ll follow the basic two steps: . Find the eigenvalues | &#39;Plug in&#39; each eigenvalue to get a system of linear equations for the values of the components of the corresponding eigenvector | Solve this linear system. | 1. Find the eigenvalues . Ok I&#39;m hoping you at least can recall what a determinant of a matrix is. Many people, even if they don&#39;t know what a determinant is good for (e.g. tons of proofs &amp; properties all rest on the determinant), they still at least remember how to calculate one. . The way to get the eigenvalues is to take the determinant of the difference between a $ bf{A}$ and $ lambda$ times the identity matrix $ bf{I}$ (which is just ones along the diagonal and zeros otherwise) and set that difference equal to zero... . $$det( bf{A} - lambda I) = 0 $$ . Just another observation:Since ${ bf I}$ is a square matrix, that means $ bf{A}$ has to be a square matrix too. Then solving for $ lambda$ will give you a polynomial equation in $ lambda$, the solutions to (or roots of) which are the eigenvectors $ lambda_i$. . Let&#39;s do an example: . $$ { bf A} = begin{bmatrix} -2 &amp; 2 &amp; 1 -5 &amp; 5 &amp; 1 -4 &amp; 2 &amp; 3 end{bmatrix} $$To find the eigenvalues we set $$ det( bf{A} - lambda I) = begin{vmatrix} -2- lambda &amp; 2 &amp; 1 -5 &amp; 5- lambda &amp; 1 -4 &amp; 2 &amp; 3- lambda end{vmatrix} = 0.$$ . This gives us the equation... $$0 = lambda^3 - 6 lambda^2 + lambda - 6$$ . which has the 3 solutions (in descending order) $$ lambda = 3, 2, 1.$$ . *(Aside: to create an integer matrix with integer eigenvalues, I used [this handy web tool](https://ericthewry.github.io/integer_matrices/))*. . Just to check that against the numpy library: . A = np.array([[-2,2,1],[-5,5,1],[-4,2,3]]) def sorted_eig(A): # For now we sort &#39;by convention&#39;. For PCA the sorting is key. lambdas, vs = LA.eig(A) # Next line just sorts values &amp; vectors together in order of decreasing eigenvalues lambdas, vs = zip(*sorted(zip(list(lambdas), list(vs.T)),key=lambda x: x[0], reverse=True)) return lambdas, np.array(vs).T # un-doing the list-casting from the previous line lambdas, vs = sorted_eig(A) lambdas # hold off on printing out the eigenvectors until we do the next part! . (3.0000000000000027, 1.9999999999999991, 1.0000000000000013) . Close enough! . 2. Use the eigenvalues to get the eigenvectors . Although it was anncounced in mid 2019 that you can get eigenvectors directly from eigenvalues, the usual way people have done this for a very long time is to go back to the matrix $ bf{A}$ and solve the linear system of equation (1) above, for each of the eigenvalues. For example, for $ lambda_1=-1$, we have . $$ { bf}A vec{v}_1 = - vec{v}_1 $$i.e. . $$ begin{bmatrix} -2 &amp; 2 &amp; 1 -5 &amp; 5 &amp; 1 -4 &amp; 2 &amp; 3 end{bmatrix} begin{bmatrix} v_{1x} v_{1y} v_{1z} end{bmatrix} = - begin{bmatrix} v_{1x} v_{1y} v_{1z} end{bmatrix} $$This amounts to 3 equations for 3 unknowns,...which I&#39;m going to assume you can handle... For the other eigenvalues things proceed similarly. The solutions we get for the 3 eigenvalues are: . $$ lambda_1 = 3: vec{v}_1 = (1,2,1)^T$$ . $$ lambda_2 = 2: vec{v}_2 = (1,1,2)^T$$ . $$ lambda_3 = 1: vec{v}_3 = (1,1,1)^T$$ . Since our original equation (1) allows us to scale eigenvectors by any artibrary constant, often we&#39;ll express eigenvectors as unit vectors $ hat{v}_i$. This will amount to dividing by the length of each vector, i.e. in our example multiplying by $(1/ sqrt{6},1/ sqrt{6},1/ sqrt{3})$. . In this setting . $$ lambda_1 = 3: hat{v}_1 = (1/ sqrt{6},2/ sqrt{6},1/ sqrt{6})^T$$ . $$ lambda_2 = 2: hat{v}_2 = (1/ sqrt{6},1/ sqrt{6},2/ sqrt{6})^T$$ . $$ lambda_3 = 1: hat{v}_3 = (1,1,1)^T/ sqrt{3}$$ . Checking our answers (left) with numpy&#39;s answers (right): . print(&quot; &quot;*15,&quot;Ours&quot;,&quot; &quot;*28,&quot;Numpy&quot;) print(np.array([1,2,1])/np.sqrt(6), vs[:,0]) print(np.array([1,1,2])/np.sqrt(6), vs[:,1]) print(np.array([1,1,1])/np.sqrt(3), vs[:,2]) . Ours Numpy [0.40824829 0.81649658 0.40824829] [-0.40824829 -0.81649658 -0.40824829] [0.40824829 0.40824829 0.81649658] [0.40824829 0.40824829 0.81649658] [0.57735027 0.57735027 0.57735027] [0.57735027 0.57735027 0.57735027] . The fact that the first one differs by a multiplicative factor of -1 is not an issue. Remember: eigenvectors can be multiplied by an arbitrary constant. (Kind of odd that numpy doesn&#39;t choose the positive version though!) . One more check: let&#39;s multiply our eigenvectors times A to see what we get: . print(&quot;A*v_1 / 3 = &quot;,np.matmul(A, np.array([1,2,1]).T)/3 ) # Dividing by eigenvalue print(&quot;A*v_2 / 2 = &quot;,np.matmul(A, np.array([1,1,2]).T)/2 ) # to get vector back print(&quot;A*v_3 / 1 = &quot;,np.matmul(A, np.array([1,1,1]).T) ) . A*v_1 / 3 = [1. 2. 1.] A*v_2 / 2 = [1. 1. 2.] A*v_3 / 1 = [1 1 1] . Great! Let&#39;s move on. Back to our data! . Eigenvectors for our sample 3D dataset . Recall we named our 3x3 covariance matrix &#39;cov&#39;. So now we&#39;ll compute its eigenvectors, and then re-plot our 3D data and also plot the 3 eigenvectors with it... . # Now that we know we can get the same answers as the numpy library, let&#39;s use it lambdas, vs = sorted_eig(cov) # Compute e&#39;vals and e&#39;vectors of cov matrix print(&quot;lambdas, vs = n&quot;,lambdas,&quot; n&quot;,vs) # Re-plot our data fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z,mode=&#39;markers&#39;, marker=dict(size=8,opacity=0.5), name=&quot;data&quot; ) ]) # Draw some extra &#39;lines&#39; showing eigenvector directions n_ev_balls = 50 # the lines will be made of lots of balls in a line ev_size= 3 # size of balls t = np.linspace(0,1,num=n_ev_balls) # parameterizer for drawing along vec directions for i in range(3): # do this for each eigenvector # Uncomment the next line to scale (unit) vector by size of the eigenvalue # vs[:,i] *= lambdas[i] ex, ey, ez = t*vs[0,i], t*vs[1,i], t*vs[2,i] fig.add_trace(go.Scatter3d(x=ex, y=ey, z=ez,mode=&#39;markers&#39;, marker=dict(size=ev_size,opacity=0.8), name=&quot;v_&quot;+str(i+1))) fig.update_layout( xaxis_title=&quot;x&quot;, yaxis_title=&quot;y&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . lambdas, vs = (1.073114351318777, 0.26724003566904386, 0.0290836286576176) [[-0.73933506 -0.47534042 -0.47690162] [-0.3717427 -0.30238807 0.87770657] [ 0.56141877 -0.82620393 -0.04686177]] . . . Things to note from the above graph: . The first (red) eigenvector points along the direction of biggest variance | The second (greenish) eigenvector points along the direction of second-biggest variance | The third (purple) eigenvector points along the direction of smallest variance. | (If you edit the above code to rescale the vector length by the eigenvector, you&#39;ll really see these three point become apparent!) . &quot;Principal Component&quot; Analysis . Now we have our components (=eigenvectors), and we have them &quot;ranked&quot; by their &quot;significance.&quot; Next we will eliminate one or more of the less-significant directions of variance. In other words, we will project the data onto the various principal components by projecting along the less-significant components. Or even simpler: We will &quot;squish&quot; the data along the smallest-variance directions. . For the above 3D dataset, we&#39;re going to squish it into a 2D pancake -- by projecting along the direction of the 3rd (purple) eigenvector onto the plane defined by the 1st (red) and 2nd (greenish) eigenvectors. . Yea, but how to do this projection? . Projecting the data . It&#39;s actually not that big of a deal. All we have to do is multiply by the eigenvector (matrix)! . OH WAIT! Hey, you want to see a cool trick!? Check this out: . lambdas, vs = sorted_eig(cov) proj_cov = vs.T @ cov @ vs # project the covariance matrix, using eigenvectors proj_cov . array([[ 1.07311435e+00, 4.47193943e-18, -4.45219967e-17], [ 5.25950058e-17, 2.67240036e-01, 6.70099547e-17], [-7.05170575e-17, 4.02835192e-17, 2.90836287e-02]]) . What was THAT? Let me clean that up a bit for you... . proj_cov[np.abs(proj_cov) &lt; 1e-15] = 0 proj_cov . array([[1.07311435, 0. , 0. ], [0. , 0.26724004, 0. ], [0. , 0. , 0.02908363]]) . Important point: What you just saw is the whole reason eigenvectors get used for so many things, because they give you a &#39;coordinate system&#39; where different &#39;directions&#39; decouple from each other. See, the system has its own (German: eigen) inherent set of orientations which are different the &#39;arbitrary&#39; coordinates that we &#39;humans&#39; may have assigned initially. . The numbers in that matrix are the covariances in the directions of the eigenvectors, instead of in the directions of the original x, y, and z. . So really all we have to do is make a coordinate transformation using the matrix of eigenvectors, and then in order to project we&#39;ll literally just drop a whole index&#39;s-worth of data-dimension in this new coordinate system. :-) . So, instead of $x$, $y$ and $z$, let&#39;s have three coordinates which (following physicist-notation) we&#39;ll call $q_1$, $q_2$ and $q_3$, and these will run along the directions given by the three eigenvectors. . Let&#39;s write our data as a N-by-3 matrix, where N is the number of data points we have. . data = np.stack((x,y,z),axis=1) data.shape # we had a 100 data points, so expecting 100x3 matrix . (100, 3) . There are two ways of doing this, and I&#39;ll show you that they&#39;re numerically equivalent: . Use all the eigenvectors to &quot;rotate&quot; the full dataset into the new coordinate system. Then perform a projection by truncating the last column of the rotated data. | Truncate the last eigenvector, which will make a 3x2 projection matrix which will project the data onto the 2D plane defined by those two eigenvectors. | Let&#39;s show them both: . print(&quot; n 1. All data, rotated into new coordinate system&quot;) W = vs[:,0:3] # keep the all the eigenvectors new_data_all = data @ W # project all the data print(&quot;Checking: new_data_all.shape =&quot;,new_data_all.shape) print(&quot;New covariance matrix = n&quot;,np.cov(new_data_all.T) ) print(&quot; n 2. Truncated data projected onto principal axes of coordinate system&quot;) W = vs[:,0:2] # keep only the first and 2nd eigenvectors print(&quot;W.shape = &quot;,W.shape) new_data_proj = data @ W # project print(&quot;Checking: new_data_proj.shape =&quot;,new_data_proj.shape) print(&quot;New covariance matrix in projected space = n&quot;,np.cov(new_data_proj.T) ) # Difference between them diff = new_data_all[:,0:2] - new_data_proj print(&quot; n Absolute maximum difference between the two methods = &quot;,np.max(np.abs(diff))) . 1. All data, rotated into new coordinate system Checking: new_data_all.shape = (100, 3) New covariance matrix = [[1.07311435e+00 7.64444687e-17 3.77081428e-17] [7.64444687e-17 2.67240036e-01 1.21906748e-16] [3.77081428e-17 1.21906748e-16 2.90836287e-02]] 2. Truncated data projected onto principal axes of coordinate system W.shape = (3, 2) Checking: new_data_proj.shape = (100, 2) New covariance matrix in projected space = [[1.07311435e+00 7.64444687e-17] [7.64444687e-17 2.67240036e-01]] Absolute maximum difference between the two methods = 0.0 . ...Nada. The 2nd method will be faster computationally though, because it doesn&#39;t calculate stuff you&#39;re going to throw away. . One more comparison between the two methods. Let&#39;s take a look at the &quot;full&quot; dataset (in blue) vs. the projected dataset (in red): . fig = go.Figure(data=[(go.Scatter3d(x=new_data_all[:,0], y=new_data_all[:,1], z=new_data_all[:,2], mode=&#39;markers&#39;, marker=dict(size=4,opacity=0.5), name=&quot;full data&quot; ))]) fig.add_trace(go.Scatter3d(x=new_data_proj[:,0], y=new_data_proj[:,1], z=new_data_proj[:,0]*0, mode=&#39;markers&#39;, marker=dict(size=4,opacity=0.5), name=&quot;projected&quot; ) ) fig.update_layout(scene_aspectmode=&#39;data&#39;) fig.show() . . . (Darn it, [if only Plot.ly would support orthographic projections](https://community.plot.ly/t/orthographic-projection-for-3d-plots/3897) [[2](https://community.plot.ly/t/orthographic-projection-instead-of-perspective-for-3d-plots/10035)] it&#39;d be a lot easier to visually compare the two datasets!) . Beyond 3D . So typically we use PCA to throw out many more dimensions than just one. Often this is used for data visualization but it&#39;s also done for feature reduction, i.e. to send less data into your machine learning algorithm. (PCA can even be used just as a &quot;multidimensional linear regression&quot; algorithm, but you wouldn&#39;t want to!) . How do you know how many dimensions to throw out? . In other words, how many &#39;components&#39; should you choose to keep when doing PCA? There are a few ways to make this judgement call -- it will involve a trade-off between accuracy and computational speed. You can make a graph of the amount of variance you get as a function of how many components you keep, and often there will be a an &#39;elbow&#39; at some point on the graph indicating a good cut-off point to choose. Stay tuned as we do the next example; we&#39;ll make such a graph. For more on this topic, see this post by Arthur Gonsales. . Example: Handwritten Digits . The scikit-learn library uses this as an example and I like it. It goes as follows: . Take a dataset of tiny 8x8 pixel images of handwritten digits. | Run PCA to break it down from 8x8=64 dimensions to just two or 3 dimensions. | Show on a plot how the different digits cluster in different regions of the space. | (This part we&#39;ll save for the Appendix: Draw boundaries between the regions and use this as a classifier.) | To be clear: In what follows, each pixel of the image counts as a &quot;feature,&quot; i.e. as a dimension. Thus an entire image can be represented as a single point in a multidimensional space, in which distance from the origin along each dimension is given by the pixel intensity. In this example, the input space is not a 2D space that is 8 units wide and 8 units long, rather it consists of 8x8= 64 dimensions. . from sklearn.datasets import load_digits from sklearn.decomposition import PCA digits = load_digits() X = digits.data / 255.0 Y = digits.target print(X.shape, Y.shape,&#39; n&#39;) # Let&#39;s look a a few examples for i in range(8): # show 8 examples print(&quot;This is supposed to be a &#39;&quot;,Y[i],&quot;&#39;:&quot;,sep=&quot;&quot;) plt.imshow(X[i].reshape([8,8])) plt.show() . (1797, 64) (1797,) This is supposed to be a &#39;0&#39;: . This is supposed to be a &#39;1&#39;: . This is supposed to be a &#39;2&#39;: . This is supposed to be a &#39;3&#39;: . This is supposed to be a &#39;4&#39;: . This is supposed to be a &#39;5&#39;: . This is supposed to be a &#39;6&#39;: . This is supposed to be a &#39;7&#39;: . Now let&#39;s do the PCA thang... First we&#39;ll try going down to 2 dimensions. This isn&#39;t going to work super great but we&#39;ll try: . digits_cov = np.cov(X.T) print(&quot;digits_cov.shape = &quot;,digits_cov.shape) lambdas, vs = sorted_eig(np.array(digits_cov)) W = vs[:,0:2] # just keep two dimensions proj_digits = X @ W print(&quot;proj_digits.shape = &quot;, proj_digits.shape) # Make the plot fig = go.Figure(data=[go.Scatter(x=proj_digits[:,0], y=proj_digits[:,1],# z=Y, #z=proj_digits[:,2], mode=&#39;markers&#39;, marker=dict(size=6, opacity=0.7, color=Y), text=[&#39;digit=&#39;+str(j) for j in Y] )]) fig.update_layout( xaxis_title=&quot;q_1&quot;, yaxis_title=&quot;q_2&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.update_layout(scene_camera=dict(up=dict(x=0, y=0, z=1), center=dict(x=0, y=0, z=0), eye=dict(x=0, y=0, z=1.5))) fig.show() . digits_cov.shape = (64, 64) proj_digits.shape = (1797, 2) . . . This is &#39;sort of ok&#39;: There are some regions that are mostly one kind of digit. But you may say there&#39;s there&#39;s too much intermingling between classes for a lot of this plot. So let&#39;s try it again with 3 dimensions for PCA: . W = vs[:,0:3] # just three dimensions proj_digits = X @ W print(&quot;proj_digits.shape = &quot;, proj_digits.shape) # Make the plot, separate them by &quot;z&quot; which is the digit of interest. fig = go.Figure(data=[go.Scatter3d(x=proj_digits[:,0], y=proj_digits[:,1], z=proj_digits[:,2], mode=&#39;markers&#39;, marker=dict(size=4, opacity=0.8, color=Y, showscale=True), text=[&#39;digit=&#39;+str(j) for j in Y] )]) fig.update_layout(title=&quot;8x8 Handwritten Digits&quot;, xaxis_title=&quot;q_1&quot;, yaxis_title=&quot;q_2&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . proj_digits.shape = (1797, 3) . . . Now we can start to see some definition! The 6&#39;s are pretty much in one area, the 2&#39;s are in another area, and the 0&#39;s are in still another, and so on. There is some intermingling to be sure (particularly between the 5&#39;s and 8&#39;s), but you can see that this &#39;kind of&#39; gets the job done, and instead of dealing with 64 dimensions, we&#39;re down to 3! . Graphing Variance vs. Components . Earlier we asked the question of how many components one should keep. To answer this quantitatively, we note that the eigenvalues of the covariance matrix are themselves measures of the variance in the datset. So these eigenvalues encode the &#39;significance&#39; that each feature-dimension has in the overall dataset. We can plot these eigenvalues in order and then look for a &#39;kink&#39; or &#39;elbow&#39; in the graph as a place to truncate our representation... . plt.plot( np.abs(lambdas)/np.sum(lambdas) ) plt.xlabel(&#39;Number of components&#39;) plt.ylabel(&#39;Significance&#39;) plt.show() . ...So, if we were wanting to represent this data in more than 3 dimensions but less than the full 64, we might choose around 10 principal compnents, as this looks like roughly where the &#39;elbow&#39; in the graph lies. . Interpretability . What is the meaning of the new coordinate axes or &#39;features&#39; $q_1$, $q_2$, etc? Sometimes there exists a compelling physical intepretation of these features (e.g., as in the case of eigenmodes of coupled oscillators), but often...there may not be any. And yet we haven&#39;t even done any &#39;real machine learning&#39; at this point! ;-) . This is an important topic. Modern data regulations such as the European Union&#39;s GDPR require that models used in algorithmic decision-making be &#39;explainable&#39;. If the data being fed into such algorithmics is already abstracted via methods such as PCA, this could be an issue. Thankfully, the linearity of the components mean that one can describe each principal component as a linear combination of inputs. . Further reading . There are many books devoted entirely to the intricacies of PCA and its applications. Hopefully this post has helped you get a better feel for how to construct a PCA transformation and what it might be good for. To expand on this see the following... . Examples &amp; links . &quot;Eigenstyle: Principal Component Analysis and Fashion&quot; by Grace Avery. Uses PCA on Fashion-MNIST. It&#39;s good! | Neat paper by my friend Dr. Ryan Bebej from when he was a student and used PCA to classify locomotion types of prehistoric acquatic mammals based on skeletal measurements alone. | Andrew Ng&#39;s Machine Learning Course, Lecture on PCA. How I first learned about this stuff. | PCA using Python by Michael Galarnyk. Does similar things to what I&#39;ve done here, although maybe better! | Plot.ly PCA notebook examples | . Appendix A: Overkill: Bigger Handwritten Digits . Sure, 8x8 digit images are boring. What about 28x28 images, as in the MNIST dataset? Let&#39;s roll... . #from sklearn.datasets import fetch_mldata from sklearn.datasets import fetch_openml from random import sample #mnist = fetch_mldata(&#39;MNIST original&#39;) mnist = fetch_openml(&#39;mnist_784&#39;, version=1, cache=True) X2 = mnist.data / 255 Y2 = np.array(mnist.target,dtype=np.int) #Let&#39;s grab some indices for random suffling indices = list(range(X2.shape[0])) # Let&#39;s look a a few examples for i in range(8): # 8 is good i = sample(indices,1) print(&quot;This is supposed to be a &quot;,Y2[i][0],&quot;:&quot;,sep=&quot;&quot;) plt.imshow(X2[i].reshape([28,28])) plt.show() . This is supposed to be a 1: . This is supposed to be a 9: . This is supposed to be a 8: . This is supposed to be a 7: . This is supposed to be a 3: . This is supposed to be a 5: . This is supposed to be a 2: . This is supposed to be a 6: . # Like we did before... Almost the whole PCA method is the next 3 lines! mnist_cov = np.cov(X2.T) lambdas, vs = sorted_eig(np.array(mnist_cov)) W = vs[:,0:3] # Grab the 3 most significant dimensions # Plotting all 70,000 data points is going to be too dense too look at. # Instead let&#39;s grab a random sample of 5,000 points n_plot = 5000 indices = sample(list(range(X2.shape[0])), n_plot) proj_mnist = np.array(X2[indices] @ W, dtype=np.float32) # Last step of PCA: project fig = go.Figure(data=[go.Scatter3d(x=proj_mnist[:,0], y=proj_mnist[:,1], z=proj_mnist[:,2], mode=&#39;markers&#39;, marker=dict(size=4, opacity=0.7, color=Y2[indices], showscale=True), text=[&#39;digit=&#39;+str(j) for j in Y2[indices]] )]) fig.show() . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: ComplexWarning: Casting complex values to real discards the imaginary part . . . ...bit of a mess. Not as cleanly separated as the 8x8 image examples. You can see that the 0&#39;s are well separated from the 1&#39;s and the 3&#39;s, but everything else is pretty mixed together. (I suspect the 1&#39;s are clustered strongly because they involve the most dark pixels.) . If you wanted to push this further then either keeping more dimensions (thereby making it un-visualizable) or just using a different method entirely (e.g. t-SNE or even better: UMAP) would be the way to go. Still, it&#39;s neat to see that you can get somewhat intelligible results in 3D even on this &#39;much harder&#39; problem. . Appendix B: Because We Can: Turning it into a Classifier . ...But let&#39;s not do a neural network because all I ever do are neural networks, and because I don&#39;t want to have take the time &amp; space to explain how they work or load in external libraries. Let&#39;s do k-nearest-neighbors instead, because it&#39;s intuitively easy to grasp and it&#39;s not hard to code up: . For any new point we want to evaluate, we take a &#39;vote&#39; of whatever some number (called $k$) of its nearest neighbor points are already assigned as, and we set the class of the new point according to that vote. . Making a efficient classifier is all about finding the *boundaries* between regions (and usually subject to some user-adjustable parameter like $k$ or some numerical threshold). Finding these boundaries can be about finding the &#39;edge cases&#39; that cause the system to &#39;flip&#39; (discontinuously) from one result to another. However, we are *not* going to make an efficient classifier today. ;-) . Let&#39;s go back to the 8x8 digits example, and split it into a training set and a testing set (so we can check ourselves): . # random shuffled ordering of the whole thing indices = sample(list(range(X.shape[0])), X.shape[0]) X_shuf, Y_shuf = X[indices,:], Y[indices] # 80-20 train-test split max_train_ind = int(0.8*X.shape[0]) X_train, Y_train = X_shuf[0:max_train_ind,:], Y_shuf[0:max_train_ind] X_test, Y_test = X_shuf[max_train_ind:-1,:], Y_shuf[max_train_ind:-1] # Do PCA on training set train_cov = np.cov(X_train.T) ell, v = sorted_eig(np.array(train_cov)) pca_dims = 3 # number of top &#39;dimensions&#39; to take W_train = v[:,0:pca_dims] proj_train = X_train @ W_train # also project the testing set while we&#39;re at it proj_test = X_test @ W_train # yes, same W_train . Now let&#39;s make a little k-nearest neighbors routine... . from collections import Counter def knn_predict(xnew, proj_train, Y_train, k=3): &quot;&quot;&quot; xnew is a new data point that has the same shape as one row of proj_train. Given xnew, calculate the (squared) distance to all the points in X_train to find out which ones are nearest. &quot;&quot;&quot; distances = ((proj_train - xnew)**2).sum(axis=1) # stick on an extra column of indexing &#39;hash&#39; for later use after we sort dists_i = np.stack( (distances, np.array(range(Y_train.shape[0]) )),axis=1 ) dists_i = dists_i[dists_i[:,0].argsort()] # sort in ascending order of distance knn_inds = (dists_i[0:k,-1]).astype(np.int) # Grab the indexes for k nearest neighbors # take &#39;votes&#39;: knn_targets = list(Y_train[knn_inds]) # which classes the nn&#39;s belong to votes = Counter(knn_targets) # count up how many of each class are represented return votes.most_common(1)[0][0] # pick the winner, or the first member of a tie # Let&#39;s test it on the first element of the testing set x, y_true = proj_test[0], Y_test[0] guess = knn_predict(x, proj_train, Y_train) print(&quot;guess = &quot;,guess,&quot;, true = &quot;,y_true) . guess = 6 , true = 6 . Now let&#39;s try it for the &#39;unseen&#39; data in the testing set, and see how we do... . mistakes, n_test = 0, Y_test.shape[0] for i in range(n_test): x = proj_test[i] y_pred = knn_predict(x, proj_train, Y_train, k=3) y_true = Y_test[i] if y_true != y_pred: mistakes += 1 if i &lt; 20: # show some examples print(&quot;x, y_pred, y_true =&quot;,x, y_pred, y_true, &quot;YAY!&quot; if y_pred==y_true else &quot; BOO. :-(&quot;) print(&quot;...skipping a lot...&quot;) print(&quot;Total Accuracy =&quot;, (n_test-mistakes)/n_test*100,&quot;%&quot;) . x, y_pred, y_true = [ 0.06075339 0.00153272 -0.0477644 ] 6 6 YAY! x, y_pred, y_true = [ 0.04083212 0.09757529 -0.05361896] 1 1 YAY! x, y_pred, y_true = [-0.0199586 -0.00778773 0.00972962] 8 5 BOO. :-( x, y_pred, y_true = [ 0.02400112 -0.07267613 0.02774141] 0 0 YAY! x, y_pred, y_true = [0.01180771 0.03483923 0.07526469] 1 7 BOO. :-( x, y_pred, y_true = [ 0.00379226 -0.06269449 -0.00195609] 0 0 YAY! x, y_pred, y_true = [-0.06832135 -0.05396545 0.02980845] 9 9 YAY! x, y_pred, y_true = [-0.02397417 -0.04914796 0.0109273 ] 5 5 YAY! x, y_pred, y_true = [ 0.08213707 -0.01608953 -0.08072889] 6 6 YAY! x, y_pred, y_true = [ 0.03056858 -0.04852946 0.02204423] 0 0 YAY! x, y_pred, y_true = [-0.02124777 0.03623541 -0.01773196] 8 8 YAY! x, y_pred, y_true = [0.03035896 0.01398381 0.01415554] 8 8 YAY! x, y_pred, y_true = [ 0.0214849 0.02114674 -0.08951798] 1 1 YAY! x, y_pred, y_true = [ 0.07878152 0.03312015 -0.06488347] 6 6 YAY! x, y_pred, y_true = [-0.01294308 0.00158962 -0.01255491] 8 5 BOO. :-( x, y_pred, y_true = [ 0.01351581 0.11000321 -0.03895516] 1 1 YAY! x, y_pred, y_true = [0.0081306 0.01683952 0.05911389] 7 1 BOO. :-( x, y_pred, y_true = [0.06497268 0.02817075 0.07118004] 4 4 YAY! x, y_pred, y_true = [-0.03879657 -0.04460611 0.02833793] 9 5 BOO. :-( x, y_pred, y_true = [-0.05975051 0.03713843 -0.07174727] 2 2 YAY! ...skipping a lot... Total Accuracy = 76.88022284122563 % . ...eh! Not bad, not amazing. You can improve the accuracy if you go back up and increase the number of PCA dimensions beyond 3, and/or increase the value of $k$. Go ahead and try it! . (For 10 dimensions and $k=7$, I got 97.7% accuracy. The highest I ever got it was 99%, but that was really working overtime, computationally speaking; the point of PCA is to let you dramatically reduce your workload while retaining reasonably high accuracy.) . Just to reiterate: This is NOT supposed to be a state of the art classifier! It&#39;s just a toy that does pretty well and helps illustrate PCA without being hard to understand or code. . The End . Thanks for sticking around! Hope this was interesting. PCA is pretty simple, and yet really useful! ...and writing this really helped me to better understand it. ;-) .",
            "url": "https://drscotthawley.github.io/blog/2019/12/21/PCA-From-Scratch.html",
            "relUrl": "/2019/12/21/PCA-From-Scratch.html",
            "date": " • Dec 21, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "My First NN Part 3. Multi-Layer Networks and Backpropagation",
            "content": "Links to lessons: Part 0, Part 1, Part 2, Part 3 . What is Backpropagation? . First watch this 5-minute video on backprop by Siraj Raval. . EDIT 2/20/2020: ^Ravel was later revealed to be plagiarizing content. I will look for an alternative link. The format in my (&quot;flipped&quot;) ML course last year involved reading things, watching brief videos, and modifying code. Siraj&#39;s video fit the bill last year as being brief &amp; good. . A Multi-Layer Network . Between the input $X$ and output $ tilde{Y}$ of the network we encountered earlier, we now interpose a &quot;hidden layer,&quot; connected by two sets of weights $w^{(0)}$ and $w^{(1)}$ as shown in the figure below. This image is a bit more complicated than diagrams one might typically encounter; I wanted to be able to show and label all the different &quot;parts.&quot; We will explain what the various symbols mean as we continue after the figure. . As before, the combined operation of weighted sum and nonlinear activation is referred to as a(n artificial) neuron; in the diagram above there are 4 &quot;hidden neurons&quot; or equivalently &quot;4 neurons in the hidden layer&quot; as well as one neuron for output. (The activations $f^{(0)}$ and $f^{(1)}$ may be the same, or they may be different.) . Semantics: What is a Layer? . The term &quot;layer&quot; in regards to neural network is not always used consistently. You may find it used in different senses by different authors. . Some users of the term will only use it with repect to weight matrices, (since these are the parts of the network which are adjusted in learning). | Others will refer to the input and (predicted) output as layers, and may or may not include the weights as layers.. | Others will only count additional &quot;hidden layers&quot; between the inputs and outputs, and these &quot;layers&quot; are connected by multiple weight matrices. | Some will speak of &quot;activation layers.&quot; In software libraries like Keras, many different types of operations and storage are referred to as layers. | For the work we&#39;ve done so far, we&#39;ve had inputs and outputs connected by one weight matrix, subject to a nonlinear activation function. Is this a two-layer network made of input and output &quot;layers,&quot;&quot; or is it a single-layer network, because there is only one weight matrix? What about the activation layer? This is to some degree a semantic issue which one does not need to get hung up on. . For our purposes it is convenient to refer to the inputs $X$, the &#39;activated&#39; hidden states $H$, and the output $ tilde{Y}$ as &quot;layers&quot;, numbering them 0, 1, and 2 respectively, and using the script notation $ mathcal{L}^l$ to denote each layer, where the layer index $l=0..2$, so that . $$ mathcal{L}^{(0)} = X, mathcal{L}^{(1)} = H, mathcal{L}^{(2)} = tilde{Y} $$This makes it easy to write the value of higher-numbered layers in terms of lower-numbed layers, i.e., $$ mathcal{L}^{(l+1)} = f^{(l)} left( { mathcal{L}^{(l)}}^T cdot w^{(l)} right), $$ where the dot $ cdot$ denotes a matrix product. This is often referred to as a &quot;feed foward&quot; operation because values are fed from left to right in the above diagram, &quot;forward&quot; through the network. (Backpropagation will involve feeding values from right to left.) . Response to student question(s): &quot;What are neurons? Like, what does this mean in terms of matrices?&quot; . This will serve as a review of the Part 1 lesson. Using the above notation, the operations from the input to the hidden layer look like this in matrix form: . ...where the lines in dark red and cyan are simply to indicate sample calculations which are part of the matrix multiplication. . Figuring out dimensions of the weights . When we learned about matrix multipliation, we remarked that most of the time in machine learning, &quot;the trick is to get the inner dimensions to match.&quot; . Let&#39;s say there are $N$ different input data &quot;points&quot; consisting of $M$ values each. So the input $X$ is an $N times M$ matrix. And let the output $ tilde{Y}$ be a $NxP$ matrix (in our example, $P=1$). If we were just connecting $X$ and $ tilde{Y}$ with no hidden layer, the single weights matrix would be a $M times P$ matrix: . $$ ( color{blue}N times color{red}M) cdot( color{red}M times color{green}P) = ( color{blue}N times color{green}P) $$(The nonlinear activation doesn&#39;t change the dimensions of the matrices.) . Adding a hidden layer with $Q$ number of neurons means we will still have $N$ different activations for each neuron (i.e. for each datapoint), so that $H$ is a $N times Q$ matrix. Thus the dimensions of $w^0$ must &quot;match&quot; between these two matrices, and so $w^0$ must be a $M times Q$ matrix: . $$ ( color{blue}N times color{red}M) cdot( color{red}M times color{purple}Q) = ( color{blue}N times color{purple}Q) $$ Similarly $w^1$ must be a $Q times P$ matrix, and the full operation in terms of matrix dimensions is . $$ ( color{blue}N times color{red}M) cdot( color{red}M times color{purple}Q) cdot( color{purple}Q times color{green}P) = ( color{blue}N times color{green}P). $$Compare this to the diagram above for $P=1$, $Q=4$. . Note: If you add bias terms to your model, you may need to remember that the number of columns in both the input $X$ and hidden layer $H$ are greater by one, i.e. $ color{red}{M} rightarrow color{red}{M+1}$, etc. . ..a bit of code . The layers $ mathcal{L}^l$ can be represented in Python a list called layers which has a of length 3. Similarly, our weights can be items in a list called weights. Returning to our first sample problem from Part 1: . import numpy as np %matplotlib inline import matplotlib.pyplot as plt # Training data: input and target X = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ]) Y = np.array([[0,0,1,1]]).T # define auxiliary variables N, M, P = X.shape[0], X.shape[1], Y.shape[1] # infer matrix shape variables from training data Y_tilde = np.zeros((N,P)) # setup storage for network output # Hidden layers Q = 4 # number of hidden neurons, i.e. &quot;size of hidden layer&quot; H = np.zeros((N,Q)) # weight matrices w0 = 2*np.random.random((M,Q)) - 1 w1 = 2*np.random.random((Q,P)) - 1 # Make lists for layers and weights layers = [X, H, Y_tilde] weights = [w0, w1] # Just try a sample calculation with random intialization to see how this works # Feed-forward (with linear activation): for l in range(2): layers[l+1] = np.dot(layers[l], weights[l]) print(&quot;layers [&quot;,l+1,&quot;] = n&quot;, layers[l+1], sep=&quot;&quot;) # sep=&quot;&quot; just omits spaces . layers [1] = [[-0.71619863 -0.93579012 0.4707394 -0.27924993] [-0.7101555 -0.1050841 0.91379718 0.29545498] [-1.16689961 -0.7912951 0.85311015 -0.32876382] [-1.16085648 0.03941092 1.29616794 0.24594109]] layers [2] = [[ 0.37453611] [-0.30449833] [-0.07951087] [-0.75854532]] . Generalizing this so it will do the full feed-forward will take a bit more code. We&#39;ll leave a placeholder routine for backpropagation for now. . # Activation choices def sigmoid(x, deriv=False): f = 1/(1+np.exp(-x)) return f*(1-f) if deriv else f def relu(x, deriv=False): return 1*(x&gt;0) if deriv else x*(x&gt;0) # Placeholder routine to perform backprop. Will fill in later def update_weights(weights, layers, Y, alpha, activ): return weights # for now, it&#39;s a no-op def fit(layers, Y, activ=[sigmoid]*2, use_bias=True, alpha=1.0, maxiter=10000): &quot;&quot;&quot; Routine for training using a multi-layer network layers: list of layer values, i.e. layers = [X, H, Y_tilde] Y: target output activ: list of activation functions. default = list of 2 sigmoids use_bias: Whether to include a constant offset in weighted sums alpha: learning rate maxiter: number of iterations to run &quot;&quot;&quot; lmax = len(layers)-1 # max index of layers, also = # of weights if use_bias: # add a column of 1&#39;s to every layer except the last for el in range(lmax): new_col = np.ones((layers[el].shape[0],1)) layers[el] = np.hstack((new_col, layers[el])) # Define weights np.random.seed(1) # for reproducibility weights = [None]*lmax # allocate slots in a blank list for el in range(lmax): # &quot;el&quot; because &quot;l&quot; and &quot;1&quot; may look similar weights[el] = 2*np.random.random((layers[el].shape[1], layers[el+1].shape[1])) - 1 loss_hist = [] # start with an empty list for iter in range(maxiter): # Feed-forward pass for el in range(lmax): layers[el+1] = activ[el](np.dot(layers[el], weights[el])) # Loss monitoring diff = layers[lmax] - Y loss_hist.append( (diff**2).mean() ) # use MSE loss for monitoring # Backprop code will go here weights = update_weights(weights, layers, Y, alpha, activ) return weights, layers[lmax], loss_hist # Test this just to make sure it runs layers = [X,H,Y_tilde] weights, Y_tilde, loss_hist = fit(layers, Y, maxiter=1) for el in range(len(weights)): print(&quot;weights[&quot;,el,&quot;] = n&quot;,weights[el], sep=&quot;&quot;) . weights[0] = [[-0.16595599 0.44064899 -0.99977125 -0.39533485 -0.70648822] [-0.81532281 -0.62747958 -0.30887855 -0.20646505 0.07763347] [-0.16161097 0.370439 -0.5910955 0.75623487 -0.94522481] [ 0.34093502 -0.1653904 0.11737966 -0.71922612 -0.60379702]] weights[1] = [[ 0.60148914] [ 0.93652315] [-0.37315164] [ 0.38464523] [ 0.7527783 ]] . ...Now that we&#39;ve achieved feed-foward operation of the network, in order to make it &#39;learn&#39; or &#39;train&#39;, we need to compare the output value $ tilde{Y}$ (which is the same as layers[2] by the way) to the target value, compute the gradients of the loss function, and then backpropagate in order to update all the weights! . Backpropagating: Theory . TL/DR: You can skip down to the last boxed equation of this section if math scares you. You will not be required to reproduce this. I do want to show you &quot;where this stuff comes from&quot;, but if you find the derivation too intimidating, you can still progress in the course just fine. . Let&#39;s review how we got the gradients for $w^{(1)}$ in Part 2, denoting weighted sums by &quot;$S$&quot;, e.g. $S^l = mathcal{L}^l cdot w^l$, we just used the Chain Rule: $$ { partial L over partial w^{(1)}} = color{blue} { partial L over partial mathcal{L}^{(2)}} color{green} { partial mathcal{L}^{(2)} over partial S^{(1)}} color{red} { partial S^{(1)} over partial w^{(1)}} $$ We&#39;ll define the first partial derivative to be $ delta^{(2)}$, which works out (given our choice of $L$ from Part 2) to be $$ color{blue}{ { partial L over partial mathcal{L}^{(2)}} = delta^{(2)} = tilde{Y}-Y},$$ i.e., it is the error in the final ouput. The next partial derivative (in green) is just the derivative of the activation function $f$, and the last partial derivative is just $ color{red}{ mathcal{L}^{(1)}}$, so as we saw in the previous lesson, we can write this &#39;schematically&#39; (i.e. not quite as a properly-set-up matix equation yet) as $$ { partial L over partial w^{(1)}} = color{blue}{ delta^{(2)}} color{green} {f^{(1) prime} } color{red}{ mathcal{L}^{(1)}} $$ whereas in proper form it will take on this ordering as a matrix equation: $$ boxed{ { partial L over partial w^{(1)}} = { mathcal{L}^{(1)}}^T cdot { delta^{(2)}} {f^{(1) prime}} }. $$ . To get the gradients for $w^{(0)}$, we can make use of a similar &quot;$ delta$&quot; notation if we&#39;re careful in how we define a new $ delta^{(1)}$. Let&#39;s write out the chain rule, and put parentheses around a particular group of terms for later: . $$ { partial L over partial w^{(0)}} = color{blue}{ left( { partial L over partial mathcal{L}^{(2)}} { partial mathcal{L}^{(2)} over partial S^{(1)}} { partial S^{(1)} over partial mathcal{L}^{(1)}} right)} color{green} { partial mathcal{L}^{(1)} over partial S^{(0)}} color{red} { partial S^{(0)} over partial w^{(0)}} $$In a manner similar to what we did above, this can be written as $$ { partial L over partial w^{(0)}} = color{blue}{ left( delta^{(2)}f^{(1) prime}w^{(1)} right)} color{green}{f^{(0) prime}} color{red}{ mathcal{L^{(0)}}} $$ We now define the terms in parentheses as $ delta^{(1)}$ . $$ color{blue}{ delta^{(1)} = delta^{(2)}f^{(1) prime}w^{(1)} }, $$...which is kind of like &quot;the error in the hidden layer,&quot; or like the final solution error projected backward into the hidden layers via our (momentarily fixed) weights $w^{(1)}$. . Then our gradients for $w^{(0)}$ take on a similar form as the gradients for $w^{(1)}$. &#39;Schematically&#39; this looks like $$ { partial L over partial w^{(0)}} = color{blue}{ delta^{(1)}} color{green}{f^{(0) prime}} color{red}{ mathcal{L^{(0)}}} $$ and in proper matrix form this is $$ boxed{ { partial L over partial w^{(0)}} = { mathcal{L}^{(0)}}^T cdot { delta^{(1)}} {f^{(0) prime}} }, $$ i.e., the same form as the preceding layer, just &quot;back&quot; one layer. We are backpropagating the errors $ delta^{(l)}$ from one layer to another in order to update the weights. . The weights are then updated as before, except now we will write this &#39;generically&#39; for all weights and layers using the index $l$: $$ boxed{ w^{(l)} := w^{(l)} - alpha { mathcal{L}^{(l)}}^T cdot { delta^{(l+1)}} {f^{(l) prime}} }, $$ where $$ delta^{(l+1)} = left { begin{array}{l} tilde{Y}-Y, &amp; &amp;l+1=l_{max} ({ rm e.g.} l_{max}=2) delta^{(l+2)}f^{(l+1) prime} cdot {w^{(l+1)}}^T, &amp; &amp;l+1 &lt; l_{max} end{array} right. $$ . Writing the Backprop Code . Now we&#39;ll use the above analysis to replace the update_weights() function from earlier. . def update_weights(weights, layers, Y, alpha, activ): &quot;&quot;&quot; Backprop routine, for arbitrary numbers of layers, assuming weights &amp; activations are defined Inputs: weights: list of arrays of weights between each layer layers: list of arrays of layer values (post-activation function) Y: target output alpha: learning rate activ: list of activation functions for each (non-input) layer Outputs: weights (updated) &quot;&quot;&quot; lmax = len(layers) - 1 # a useful variable assert len(weights)==lmax # make sure number of weights match up assert len(activ) &gt;= lmax # make sure we defined enough activations for the layers delta = layers[lmax] - Y # error between output and target for el in range(lmax-1, -1, -1): # Count backwards to layer zero fprime = activ[el](np.dot(layers[el], weights[el]), deriv=True) # deriv of activation weights[el] -= alpha * np.dot( layers[el].T, delta*fprime ) # gradient descent step delta = np.dot(delta*fprime, weights[el].T) if (el&gt;0) else None # setup delta for next pass in loop return weights # Let&#39;s run it! layers = [X, H, Y_tilde] alpha = 0.1 maxiter=5000 weights, Y_tilde, loss_hist_2weights = fit(layers, Y, alpha=alpha, maxiter=maxiter) # compare against a 1-weight (no hidden layer) network: layers = [X, Y_tilde] weights, Y_tilde, loss_hist_1weight = fit(layers, Y, alpha=alpha, maxiter=maxiter) # Plot the loss history plt.semilogy(loss_hist_1weight, label=&quot;No hidden layers&quot;) plt.semilogy(loss_hist_2weights, label=&quot;Hidden layer&quot;) plt.legend() plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.show() . # Let&#39;s add more hidden neurons Q = 50 # number of hidden neurons, i.e. &quot;size of hidden layer&quot; H = np.zeros((N,Q)) layers = [X, H, Y_tilde] weights, Y_tilde, loss_hist_many = fit(layers, Y, alpha=alpha, maxiter=maxiter) # try a relu activation for the hidden layer (leave output activ as sigmoid!) weights, Y_tilde, lhm_relu = fit(layers, Y, alpha=alpha, activ=[relu,sigmoid], maxiter=maxiter) plt.semilogy(loss_hist_1weight, label=&quot;0 hidden neurons&quot;) plt.semilogy(loss_hist_2weights, label=&quot;4 hidden neurons&quot;) plt.semilogy(loss_hist_many, label=&quot;many hidden neurons&quot;) plt.semilogy(lhm_relu, label=&quot;many, relu on hidden&quot;) plt.legend() plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) . Text(0, 0.5, &#39;Loss&#39;) . Solving XOR . Now let&#39;s revisit the &quot;XOR&quot; problem that a single neuron couldn&#39;t handle. . $$ overbrace{ left[ { begin{array}{cc} 0 &amp; 0 0 &amp; 1 1 &amp; 0 1 &amp; 1 end{array} } right] }^{X} rightarrow overbrace{ left[ { begin{array}{c} 0 1 1 0 end{array} } right] }^Y. $$With our multi-layer network, we can solve this. Note that while an exact solution to the XOR problem exists using only 2 hidden neurons and linear activations, a program can still have a hard time finding a good approximation via gradient descent, and we use 20 hidden neurons to assist, as follows: . X = np.array([[0,0],[0,1],[1,0],[1,1]]) Y = np.array([[0,1,1,0]]).T Y_tilde = 0*Y # Just allocate some storage H = np.zeros((N,20)) weights, Y_tilde, loss_hist_xor = fit([X,H,Y_tilde], Y, activ=[relu,sigmoid], alpha=0.5) print(&quot;Prediction Y_tilde =&quot;,Y_tilde.T) print(&quot;Target Y (correct answer) =&quot;,Y.T) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.loglog(loss_hist_xor) #print(&quot;weights = &quot;,weights) . Prediction Y_tilde = [[0.00480128 0.9962425 0.9963109 0.00374649]] Target Y (correct answer) = [[0 1 1 0]] . [&lt;matplotlib.lines.Line2D at 0x7f9283c17550&gt;] . Same thing using neural network libraries Keras &amp; PyTorch. . Since most of the time we won&#39;t be writing neural network systems &quot;from scratch, by hand&quot; in numpy, let&#39;s take a look at similar operations using libraries such as Keras or PyTorch. . Keras version . Keras is so simple to set up, it&#39;s easy to get started. This is what the previous example for XOR looks like &quot;in Keras&quot;: . import keras from keras.models import Sequential from keras.layers import Dense, Activation from keras.optimizers import Adam # training data X = np.array([[0,0],[0,1],[1,0],[1,1]]) Y = np.array([[0,1,1,0]]).T # specify model n_hidden = 20 model = Sequential([ Dense(n_hidden, input_shape=(X.shape[1],), activation=&#39;relu&#39;), Dense(1, activation=&#39;sigmoid&#39;)]) # choices for loss and optimization method opt = Adam(lr=alpha) # We&#39;ll talk about optimizer choices later model.compile(optimizer=opt, loss=&#39;binary_crossentropy&#39;,metrics=[&#39;binary_accuracy&#39;]) # training iterations model.fit(X, Y, epochs=maxiter, batch_size=1, verbose=0) print(&quot; nY_tilde = n&quot;, model.predict(X) ) . Y_tilde = [[0.00] [1.00] [1.00] [0.00]] . Keras can get a better appoximation than we did because of the choice of optimizer algorithm. We&#39;ll talk about optimization algorithms (refinements to gradient descent) another time. . PyTorch version . Unlike Keras, PyTorch does not have any &quot;training wheels.&quot; You have to specify a number of the operations yourself. It&#39;s helpful to have a template to start from, such as the following example for our XOR problem. . import torch # it&#39;s &#39;PyTorch&#39; but the package is &#39;torch&#39; device = torch.device(&#39;cpu&#39;) # handy for changing to &#39;cuda&#39; in GPU runtimes later! torch.manual_seed(1) # for reproducibility # training data X = np.array([[0,0],[0,1],[1,0],[1,1]],dtype=np.float32) Y = np.array([[0,1,1,0]],dtype=np.float32).T # re-cast data as PyTorch variables, on the device (CPU or GPU) were calc&#39;s are performed x, y = torch.tensor(X).to(device), torch.tensor(Y).to(device) # specify model (similar to Keras but not quite) n_hidden = 20 # number of hidden neurons model = torch.nn.Sequential( torch.nn.Linear(X.shape[1], n_hidden), torch.nn.ReLU(), torch.nn.Linear(n_hidden, 1), torch.nn.Sigmoid() ).to(device) # choices for loss and optimization method loss_fn = torch.nn.BCELoss() # binary cross-entropy loss optimizer = torch.optim.Adam([{&#39;params&#39;: model.parameters()}], lr=alpha) # training iterations loss_hist_pytorch = [] for iter in range(maxiter): optimizer.zero_grad() # set gradients=0 before calculating more y_tilde = model(x) # feed-forward step loss = loss_fn(y_tilde, y) # compute the loss loss_hist_pytorch.append(loss.item()) # save loss for plotting later loss.backward() # compute gradients via backprop optimizer.step() # actually update the weights # print and plot our results print(&quot; nY_tilde = n&quot;, y_tilde.cpu().data.numpy() ) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.loglog(loss_hist_pytorch) . Y_tilde = [[2.9910339e-07] [9.9999976e-01] [9.9999964e-01] [3.4460226e-07]] . [&lt;matplotlib.lines.Line2D at 0x7f38a9fcfe80&gt;] . Exercise: Exploring Hidden Layers. . More with the 7-segment display . Using the $X$ and $Y$ arrays for from previous exercices with the 7-segment display, we&#39;ll explore the effects of adding hidden neurons and different activation functions. Using the code template that follows below,... . A. Explore hidden layer sizes &amp; activations . Write code for a new activation function: $ tanh(x)$ and its derivative. Note: there is already placeholder code for this in the template below | Set training data to be that of the 7-segment display. | Choose (for yourself) a single learning rate (e.g. $ alpha=$0.5), and a standard number of iterations (e.g. 10000). | Then compare results for multiple networks (all with softmaxsigmoid activation on the end): . A single hidden layer with 20 neurons and (for the hidden layer)... . sigmoid activation | relu activation | tanh activation | . | A single hidden layer with 100 neurons and (for the hidden layer)... . sigmoid activation | relu activation | tanh activation | . | B. Explore multiple hidden layers . Now use two hidden layers, H and H2 with 10 neurons each, and experiment to find the best combination of activations, and best choice of learning rate that gives you the lowest loss at the end of your chosen number of iterations. Check that your predicted output is as you expect. | Assignment: . Upload a text file of the code for your &quot;winning&quot; entry for #3 to Blackboard. Use the code below as a template. . ## TEMPLATE CODE. Scroll down to &quot;MAKE YOUR CHANGES BELOW&quot;, below import numpy as np import matplotlib.pyplot as plt ### LEAVE THIS UNCHANGED # First, let&#39;s repeat the sigmoid(), relu(), update_weights() and fit() routines # already defined, so we have a&#39;standalone&#39; code and can easily make changes # Activation choices def sigmoid(x, deriv=False): f = 1/(1+np.exp(-x)) return f*(1-f) if deriv else f def relu(x, deriv=False): return 1*(x&gt;0) if deriv else x*(x&gt;0) # Backpropagation routine def update_weights(weights, layers, Y, alpha, activ): lmax = len(layers) - 1 # a useful variable assert len(weights)==lmax # make sure number of weights match up assert len(activ) &gt;= lmax # make sure we defined enough activations for the layers delta = layers[lmax] - Y # error between output and target for el in range(lmax-1, -1, -1): # Count backwards to layer zero fprime = activ[el](np.dot(layers[el], weights[el]), deriv=True) # deriv of activation weights[el] -= alpha * np.dot( layers[el].T, delta*fprime ) # gradient descent step delta = np.dot(delta*fprime, weights[el].T) if (el&gt;0) else None # setup delta for next pass in loop return weights # Routine for training via gradient descent def fit(layers, Y, activ=[sigmoid]*2, use_bias=True, alpha=1.0, maxiter=10000): lmax = len(layers)-1 # max index of layers, also = # of weights if use_bias: # add a column of 1&#39;s to every layer except the last for el in range(lmax): new_col = np.ones((layers[el].shape[0],1)) layers[el] = np.hstack((new_col, layers[el])) # Define weights np.random.seed(1) # for reproducibility weights = [None]*lmax # allocate slots in a blank list for el in range(lmax): # &quot;el&quot; because &quot;l&quot; and &quot;1&quot; may look similar weights[el] = 2*np.random.random((layers[el].shape[1], layers[el+1].shape[1]))-1 loss_hist = [] # start with an empty list for iter in range(maxiter): # Feed-forward pass for el in range(lmax): layers[el+1] = activ[el](np.dot(layers[el], weights[el])) Y_tilde = layers[lmax] # Loss monitoring diff = Y_tilde - Y loss_hist.append( (diff**2).mean() ) # use MSE loss for monitoring # Backprop code will go here weights = update_weights(weights, layers, Y, alpha, activ) return weights, Y_tilde, loss_hist ##### END OF PART TO LEAVE UNCHANGED ##### MAKE YOUR CHANGES BELOW ############## # define the tanh activation function def tanh(x, deriv=False): if deriv: pass # *** Students: replace &#39;pass&#39; with what the derivative should be return np.tanh(x) ## Students: replace X, Y with 7-segment data instead X = np.array([[0,0],[0,1],[1,0],[1,1]],dtype=np.float32) Y = np.array([[0,1,1,0]],dtype=np.float32).T Y_tilde = np.copy(Y) # Just allocates some storage for Y_tilde ## Hidden layers: Students: Change Q, the number of hidden neurons, as needed Q = 10 N = X.shape[0] # this just grabs the number of rows in X H = np.zeros((N,Q)) H2 = np.zeros((N,Q)) # extra hidden layer, might not be used ## Students: change this as instructed layers = [X, H, Y_tilde] # later, add another layer H2 when instructed activ = [sigmoid, sigmoid, sigmoid] # change the first (2) activation(s) as instructed alpha = 0.5 # play around with this ## LEAVE THIS PART UNCHANGED weights, Y_tilde, loss_hist = fit(layers, Y, activ=activ, alpha=alpha) np.set_printoptions(formatter={&#39;float&#39;: lambda x: &quot;{0:0.2f}&quot;.format(x)}) # 2 sig figs print(&quot;Prediction Y_tilde = n&quot;,Y_tilde.T) print(&quot;Target Y (correct answer) = n&quot;,Y.T) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.loglog(loss_hist) . Prediction Y_tilde = [[0.01 0.99 0.99 0.01]] Target Y (correct answer) = [[0.00 1.00 1.00 0.00]] . [&lt;matplotlib.lines.Line2D at 0x7f928397cf98&gt;] . Preview of next lesson: MNIST . Now that you&#39;ve built up some experience with reading digits, let&#39;s move to handwritten digits! This is a problem usually solved with an architecture called a Convolutional Neural Network, but our ordinary feed-forward network can do it too. . The MNIST database of handwritten digits is a classic dataset that every ML student works on. It consists of a large number images of handwritten digits only 28x28 pixels in size. We will &quot;flatten&quot; these into a row of 784 columns, and output a $ tilde{Y}$ of one-hot-encoded vectore just like we did for the output of the 7-segment display (same digits, 0 to 9!). .",
            "url": "https://drscotthawley.github.io/blog/2019/02/08/My-1st-NN-Part-3-Multi-Layer-and-Backprop.html",
            "relUrl": "/2019/02/08/My-1st-NN-Part-3-Multi-Layer-and-Backprop.html",
            "date": " • Feb 8, 2019"
        }
        
    
  
    
        ,"post7": {
            "title": "My First Neural Network, Part 2. Bias and CE Loss",
            "content": "Links to lessons: Part 0, Part 1, Part 2, Part 3 . Moving on from our our previous notebook, we will investigate three things we could do to improve the models developed previously: . Add a bias term | Use a different loss function | Add more layers to the network (postponed to next lesson) | 1. Add a bias term . Our weighted sums did not include any constant offset or &quot;bias&quot; term. This may be fine for some data, but not for many others. For example, in a simple linear model $y = mx+b$, the choice of $b=0$ limits the model&#39;s ability to accurately fit some data. . . That is effectively what we did with our weighted sum $ sum_j X_{ij}w_j$: there was no constant offset. To correct this lack, we could add a new variable $b$ and make our weighted sum $b + sum_j X_{ij}w_j$. Equivalently, and more conveniently for the purposes of coding, we could put an additional column of 1&#39;s in the input $X$, and a new row to our weight matrix $w$. By convention, this is usually done with the zeroth element, so that $X_{i0}=1$ and the columns of $X$ are moved to the right, and $w_0 = b$ will be the new constant offset (because $1*w_0 = w_0$.) . For the first problem (Trask&#39;s first problem), this change makes our new matrix equation look like (with new bias terms in red) . $$ f left( overbrace{ left[ { begin{array}{ccc} color{red}1 &amp; 0 &amp; 0 &amp; 1 color{red}1 &amp; 0 &amp; 1 &amp; 1 color{red}1 &amp; 1 &amp; 0 &amp; 1 color{red}1 &amp; 1 &amp; 1 &amp; 1 end{array} } right] }^ text{X} overbrace{ left[ { begin{array}{c} color{red}{w_0} w_1 w_2 w_3 end{array} } right] }^{w} right) = overbrace{ left[ { begin{array}{c} 0 0 1 1 end{array} } right] }^{ tilde{Y}} $$Foreshadowing: Note that in this problem, the rightmost column of $X$ already was a column of all 1&#39;s, and so already has something akin to a bias. Thus, adding a new column of all 1&#39;s will not add any information, and so for this problem we expect that adding the bias won&#39;t improve the model performance.) . With this change, we can still write our weighted sum as $ sum_j X_{ij}w_j$, it&#39;s just that $j$ now runs over 0..3 instead of 0..2. To emphasize: We can leave the rest of our code the same as before, provided we change $X$ by adding a column of 1&#39;s. . In terms of coding the change to $X$, we can either rewrite it by hand, or pull a numpy trick: . import numpy as np import matplotlib.pyplot as plt %matplotlib inline # old data X = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ]) # add a column of 1&#39;s new_col = np.ones((X.shape[0],1)) # array of 1s, w/ same # of rows as X, 1 col wide X_bias = np.hstack((new_col,X)) # stack them horizontally print(X_bias) . [[1. 0. 0. 1.] [1. 0. 1. 1.] [1. 1. 0. 1.] [1. 1. 1. 1.]] . Let&#39;s compare our the use of the bias term without. We&#39;ll define functions for the gradient descent and for the plotting of the loss history, so we can call these again later in this lesson. . Y = np.array([[0,0,1,1]]).T # target output dataset def sigmoid(x,deriv=False): if(deriv==True): return x*(1-x) return 1/(1+np.exp(-x)) def calc_loss(Y_pred, Y, X, w, activ, loss_type=&quot;mse&quot;): # MSE loss diff = Y_pred - Y loss = (diff**2).mean() gradient = np.dot( X.T, diff*activ(Y_pred, deriv=True)) # for weight update return loss, gradient def fit(X, Y, activ, use_bias=True, alpha=3.0, maxiter=10000, loss_type=&#39;mse&#39;): &quot;&quot;&quot; fit: Generic routine for doing our gradient decent Required arguments: X: input matrix Y: target output activ: reference to an activation function Keywork arguments (optional): use_bias: Flag for whether to use bias in the model alpha: learning rate. Tip: Use the largest alpha &#39;you can get away with&#39; maxiter: maximum number of iterations loss_type: Set to MSE for now but we&#39;ll extend this later. &quot;&quot;&quot; if use_bias: # add a column of 1&#39;s to X new_col = np.ones((X.shape[0],1)) X = np.hstack((new_col,X)) # Define weights np.random.seed(1) # for reproducibility if activ == sigmoid: w = 2*np.random.random((X.shape[1],Y.shape[1])) - 1 # -1..1 else: w = np.random.random((X.shape[1],Y.shape[1]))/10 # only positive weights (for later) loss_hist = [] # start with an empty list for iter in range(maxiter): Y_pred = activ(np.dot(X,w)) # compute prediction, i.e. tilde{Y} loss, gradient = calc_loss(Y_pred, Y, X, w, activ, loss_type) loss_hist.append(loss) # add to the history of the loss w -= alpha * gradient # weight update return w, Y_pred, loss_hist # send these values back # Now call the fit function twice, to compare with and without bias: w_old, Y_pred_old, loss_hist_old = fit(X, Y, sigmoid, use_bias=False) w_new, Y_pred_new, loss_hist_new = fit(X, Y, sigmoid) # Plot the results. Make a function so we can call this again later def plot_new_old(loss_hist_old, loss_hist_new, labels=[&quot;no bias&quot;, &quot;with bias&quot;]): plt.loglog(loss_hist_old, label=labels[0]) plt.loglog(loss_hist_new, label=labels[1]) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;MSE Loss (monitoring)&quot;) plt.legend() plt.show() plot_new_old(loss_hist_old, loss_hist_new) # And print the final answers: print(&quot;No bias: Y_pred =&quot;,Y_pred_old) print(&quot;With bias: Y_pred =&quot;,Y_pred_new) . No bias: Y_pred = [[0.00553557] [0.00451069] [0.99632065] [0.9954838 ]] With bias: Y_pred = [[0.00485703] [0.00433876] [0.99612094] [0.99565733]] . As expected, for this problem, the inclusion of bias didn&#39;t make any significant difference. Let&#39;s try the same thing for the 7-segment display problem from Part 1. And let&#39;s try two different runs, one with sigmoid activation, and another with ReLU: . X_7seg = np.array([ [1,1,1,1,1,1,0,1], # 0 [0,1,1,0,0,0,0,1], # 1 [1,1,0,1,1,0,1,1], # 2 [1,1,1,1,0,0,1,1], # 3 [0,1,1,0,0,1,1,1], # 4 [1,0,1,1,0,1,1,1], # 5 [1,0,1,1,1,1,1,1], # 6 [1,1,1,0,0,0,0,1], # 7 [1,1,1,1,1,1,1,1], # 8 [1,1,1,1,0,1,1,1] # 9 ]) Y_7seg = np.eye(10) X, Y = X_7seg, Y_7seg def relu(x,deriv=False): # relu activation if(deriv==True): return 1*(x&gt;0) return x*(x&gt;0) # Call the fit routine twice, once for sigmoid activation, once for relu for activ in [sigmoid, relu]: print(&quot; n n activ = &quot;,activ) alpha = 0.5 if activ == sigmoid else 0.005 # assign learning rate w_old, Y_pred_old, loss_hist_old = fit(X, Y, activ, alpha=alpha, use_bias=False) w_new, Y_pred_new, loss_hist_new = fit(X, Y, activ, alpha=alpha) # Report results plot_new_old(loss_hist_old, loss_hist_new) np.set_printoptions(formatter={&#39;float&#39;: lambda x: &quot;{0:0.2f}&quot;.format(x)}) # 2 sig figs print(&quot;No bias: Y_pred = n&quot;,Y_pred_old) print(&quot;With bias: Y_pred = n&quot;,Y_pred_new) . activ = &lt;function sigmoid at 0x7fa465235d08&gt; . No bias: Y_pred = [[0.98 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.03 0.02] [0.01 0.98 0.00 0.01 0.01 0.00 0.00 0.02 0.00 0.00] [0.00 0.00 0.99 0.01 0.00 0.00 0.01 0.00 0.03 0.00] [0.00 0.00 0.01 0.98 0.00 0.00 0.00 0.01 0.00 0.02] [0.00 0.01 0.00 0.00 0.99 0.00 0.00 0.00 0.03 0.02] [0.00 0.00 0.00 0.01 0.01 0.98 0.02 0.00 0.00 0.02] [0.01 0.00 0.00 0.00 0.00 0.01 0.98 0.00 0.03 0.00] [0.01 0.01 0.00 0.01 0.00 0.00 0.00 0.98 0.00 0.00] [0.01 0.00 0.01 0.00 0.00 0.00 0.01 0.00 0.94 0.00] [0.00 0.00 0.00 0.01 0.01 0.01 0.00 0.00 0.02 0.96]] With bias: Y_pred = [[0.98 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.03 0.01] [0.01 0.98 0.00 0.01 0.01 0.00 0.00 0.02 0.00 0.00] [0.00 0.00 0.99 0.01 0.00 0.00 0.01 0.00 0.02 0.00] [0.00 0.00 0.01 0.98 0.00 0.00 0.00 0.01 0.00 0.02] [0.00 0.01 0.00 0.00 0.99 0.00 0.00 0.00 0.02 0.02] [0.00 0.00 0.00 0.01 0.01 0.98 0.02 0.00 0.00 0.02] [0.01 0.00 0.00 0.00 0.00 0.01 0.98 0.00 0.03 0.00] [0.01 0.01 0.00 0.01 0.00 0.00 0.00 0.98 0.00 0.00] [0.01 0.00 0.01 0.00 0.00 0.00 0.01 0.00 0.95 0.01] [0.00 0.00 0.00 0.01 0.01 0.01 0.00 0.00 0.01 0.97]] activ = &lt;function relu at 0x7fa4651f2400&gt; . No bias: Y_pred = [[1.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.00 0.01 0.00] [-0.00 1.00 -0.00 -0.00 0.00 -0.00 -0.00 0.00 -0.00 -0.00] [-0.00 -0.00 1.00 0.00 -0.00 -0.00 -0.00 -0.00 0.01 -0.00] [-0.00 -0.00 -0.00 1.00 -0.00 -0.00 -0.00 0.00 -0.00 0.00] [-0.00 0.00 -0.00 -0.00 1.00 -0.00 -0.00 -0.00 0.01 0.00] [-0.00 -0.00 -0.00 -0.00 -0.00 1.00 0.00 -0.00 -0.00 0.00] [-0.00 -0.00 -0.00 -0.00 -0.00 0.00 1.00 -0.00 0.01 -0.00] [0.00 0.00 -0.00 0.00 -0.00 -0.00 -0.00 1.00 -0.00 -0.00] [0.00 -0.00 0.00 -0.00 -0.00 -0.00 0.00 -0.00 0.96 -0.00] [-0.00 -0.00 -0.00 0.00 0.00 0.00 -0.00 -0.00 0.00 1.00]] With bias: Y_pred = [[1.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.00 0.00 0.00] [-0.00 1.00 -0.00 -0.00 0.00 -0.00 -0.00 0.00 -0.00 -0.00] [-0.00 -0.00 1.00 0.00 -0.00 -0.00 -0.00 -0.00 0.00 -0.00] [-0.00 -0.00 -0.00 1.00 -0.00 -0.00 -0.00 0.00 -0.00 0.00] [-0.00 0.00 -0.00 -0.00 1.00 -0.00 -0.00 -0.00 0.00 0.00] [-0.00 -0.00 -0.00 -0.00 -0.00 1.00 0.00 -0.00 -0.00 0.00] [-0.00 -0.00 -0.00 -0.00 -0.00 0.00 1.00 -0.00 0.00 -0.00] [0.00 0.00 -0.00 0.00 -0.00 -0.00 -0.00 1.00 -0.00 -0.00] [0.00 -0.00 0.00 -0.00 -0.00 -0.00 0.00 -0.00 0.99 -0.00] [-0.00 -0.00 -0.00 0.00 0.00 0.00 -0.00 -0.00 0.00 1.00]] . ...So for this problem, it seems that adding the bias gave us a bit more accuracy, both for the sigmoid and relu activations. Note: in this example, the learning rates were chosen by experiment; you should get in the habit of going back and experimenting with different learning rates. . Video Interlude: Logistic Regression . What we&#39;ve been doing up until now has been a &quot;classification&quot; problem, with &quot;yes&quot;/&quot;no&quot; answers represented by 1&#39;s and 0&#39;s. This sort of operation is closely associated with the statistical method of Logistic Regression. It is akin to linear regression but with a sigmoid activation function. When doing Logistic Regression, one optimizes to fit by finding the maximum &quot;likelihood&quot; of a given model being correct. . To gain some insight on Logistic Regression, watch this SatsQuest video. (You can ignore his remarks about his preceding video, &quot;R squared&quot; and &quot;p-value&quot;, etc.) . In what follows, we will be minimizing the negative of the logarithm of the likelihood, a quantity typically known as the Cross-Entropy loss. (This same quantity is also the non-constant part of the &quot;Kullback-Leibler Divergence&quot; or &quot;K-L divergence,&quot; so you may hear it called that sometimes.) . 2. Use a different loss function: Cross-Entropy loss . Let&#39;s return to Trask&#39;s first problem for which there is only one target per data point (row) of input, namely a target of 0 or 1. In this case, using the sigmoid function for this classification problem is one of logistic regression, even though we hadn&#39;t it identified it as such. . We&#39;ve been using mean squared error (MSE) loss, but other loss functions exist. In particular, for outputs which are either &quot;yes&quot; or &quot;no&quot; such as the classification problem we are solving, a function called &quot;cross entropy&quot; is typically preferred. The cross-entropy loss is written like this: . $$L_{CE} = - sum_i left[ Y_i log( tilde{Y}_i) + (1-Y_i) log(1- tilde{Y}_i) right]$$ . Note that since the function $ log(x)$ is undefined for $x le0$, we need to make sure $0&lt; tilde{Y}_i&lt;1$ for all $i$. One way to ensure this is to use sigmoid activation! Thus, for classification problems, it is very common to see sigmoid activation (or its multi-class relative &quot;softmax&quot;) immediately before the output, even for many-layer neural networks with all kinds of other activations in other places. . To use the CE loss with gradient descent, we need its derivative with respect to the weights. First let&#39;s write the CE loss in terms of the inputs $X$, weights $w$ and activation function $f$: . ...wait, for compactness, let&#39;s write the weighted sum as $S_i = sum_j X_{ij}w_j$. Ok, now going forward.... . $$L_{CE} = - sum_i left[ Y_i log left(f left(S_i right) right) + (1-Y_i) log left(1- f left(S_{i} right) right) right]$$ . For any function $g(x)$, the derivative of $ log(g(x))$ with respect to x is just $1/g*(du/dx)$, so our partial derivatives with respect to weights look like . $${ partial L_{CE} over partial w_j} = - sum_i left[ {Y_i over tilde{Y_i}}{ partial f(S_i) over partial w_j} - {1-Y_i over 1- tilde{Y}_i} { partial f(S_i) over partial w_j} right] = - sum_i { partial f(S_i) over partial S_i}{ partial S_i over partial w_j} left[ {Y_i over tilde{Y_i}} - {1-Y_i over 1- tilde{Y}_i} right] $$And if we multiply by $2/N$, we can write this as $$ { partial L_{CE} over partial w_j} = {2 over N} sum_{i=0}^{N-1} { partial f(S_i) over partial S_i}X_{ij} left[ { tilde{Y_i}-Y_i over tilde{Y_i}(1- tilde{Y_i}) } right]$$ This is similar to the partial derivatives for our MSE loss, except the term in the denominator is new. To see this more clearly, recall that the weight update for MSE (from Part 1) was $$ w := w - alpha X^T cdot left( [ tilde{Y}-Y]* tilde{Y}*(1- tilde{Y}) right) $$ whereas for CE we actually get a bit of a simplification because the term in the denominator cancels with a similar term in the numerator: $$ w := w - alpha X^T cdot left( [ tilde{Y}-Y]* tilde{Y}*(1- tilde{Y}) right) / ( tilde{Y}*(1- tilde{Y})) w := w - alpha X^T cdot [ tilde{Y}-Y]. $$ Thus despite all this seeming complication, our CE weight update is actually simpler than what it was before as MSE! . Let&#39;s try this out with code now: . # we&#39;ll &quot;overwrite&quot; the earlier calc_loss function def calc_loss(Y_pred, Y, X, w, activ, loss_type=&#39;ce&#39;): diff = Y_pred - Y loss = (diff**2).mean() # MSE loss if &#39;ce&#39; == loss_type: diff = diff / (Y_pred*(1-Y_pred)) # use this for gradient #loss = -(Y*np.log(Y_tilde) + (1-Y)*np.log1p(-Y_tilde)).mean() # CE Loss # Actually we don&#39;t care what the loss itself is. # Let&#39;s use MSE loss for &#39;monitoring&#39; regardless, so we can compare the # effects of using different gradients-of-loss functions gradient = np.dot( X.T, diff*activ(Y_pred, deriv=True)) # same as before return loss, gradient #- X = X_bias Y = np.array([[0,0,1,1]]).T # target output dataset # Compare old and new w_mse, Y_pred_mse, loss_hist_mse = fit(X, Y, sigmoid, alpha=0.5, loss_type=&#39;mse&#39;) w_ce, Y_pred_ce, loss_hist_ce = fit(X, Y, sigmoid, alpha=0.5, loss_type=&#39;ce&#39;) # fit plot_new_old(loss_hist_mse, loss_hist_ce, [&quot;MSE, with bias&quot;, &quot;CE, with bias&quot;]) # And print the final answers: print(&quot;MSE _loss: Y_pred = n&quot;,Y_pred_mse) print(&quot;CE loss: Y_pred = n&quot;,Y_pred_ce) . MSE _loss: Y_pred = [[0.01] [0.01] [0.99] [0.99]] CE loss: Y_pred = [[0.00] [0.00] [1.00] [1.00]] . This works a lot better! To understand why, note that the gradients for MSE loss scale like $$[ tilde{Y}-Y]* tilde{Y}*(1- tilde{Y})$$ and thus these gradients go to zero as $ tilde{Y} rightarrow 0$, and/or $ tilde{Y} rightarrow 1$, which makes training very slow! In contrast, the extra denominator in the CE gradients effectively cancels out this behavior, leaving the remaining term of $$[ tilde{Y}-Y]$$ which varies linearly with the difference from the target value. This makes training much more efficient. . # Aside: What happens if we try ReLU activation with CE loss? Bad things, probably. # Recall that ReLU maps negative numbers to 0, and isn&#39;t bounded from above. # Thus the &quot;denominator&quot; in the &#39;diff term&#39; in our earlier code will tend to &#39;explode&#39;. # Put differently, note that log(x) is undefined for x=0, as is log(1-x) for x=1. w_relu_ce, Y_pred_relu_ce, loss_hist_relu_ce = fit(X, Y, relu, alpha=0.001, loss_type=&#39;ce&#39;) plot_new_old(loss_hist_ce, loss_hist_relu_ce, [&quot;CE, sigmoid&quot;, &quot;CE, ReLU&quot;]) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in greater /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in greater . Excercise: . Do the same comparison for the 7-segment display problem: Make a plot showing a comparison of the loss history use MSE loss vs. using CE loss. And print out the final values of Y_pred for each. Use a learning rate of 0.5 and sigmoid activation, with bias. . Take a screenshot of the output and upload it to your instructor. . (Note: for the 7-segment display, since the target $Y$ has multiple columns, we should &quot;normalize&quot; the output in order to be able to properly interpret the output values $ tilde{Y}$ as probabilities. To do so, we would use a softmax activation. For now, we haven&#39;t bothered with this because it would add a bit more math, and is not actually necessary to solve this problem. ) . Next time: Part 3: Multi-Layer Networks and Backpropagation .",
            "url": "https://drscotthawley.github.io/blog/2019/02/04/My-First-NN-Part-2.html",
            "relUrl": "/2019/02/04/My-First-NN-Part-2.html",
            "date": " • Feb 4, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "My First Neural Network, Part 1",
            "content": "Links to lessons: Part 0, Part 1, Part 2 Part 3 . We will be reproducing work from Andrew Trask&#39;s excellent tutorial &quot;A Neural Network in 11 lines of Python&quot;, albeit with a different emphasis, and in a different way. You may regard this treatment and his original treatment as complimentary, and feel free to refer to both. This lesson is written with the intent of building on the lesson about linear regression -- which we might call &quot;Part 0&quot; -- at the link &quot;Following Gravity - ML Foundations Part Ia.&quot; . The Sample Problem . Consider a system that tries to map groups of 3 inputs to some corresponding output which is a single number. In the following picture, we&#39;ll show each set of 3 inputs as a row of a matrix $X$, and each output as the corresponding row of $Y$: . $$ overbrace{ left[ { begin{array}{ccc} 0 &amp; 0 &amp; 1 0 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 end{array} } right] }^{X} rightarrow overbrace{ left[ { begin{array}{c} 0 0 1 1 end{array} } right] }^Y. $$Even though this system has an exact solution (namely, $Y$ equals the first column of $X$), usually we&#39;ll need to be satisfied with a system that maps our inputs $X$ to some approximate &quot;prediction&quot; $ tilde{Y}$, which we hope to bring closer to the &quot;target&quot; $Y$ by means of successive improvements. . The way we&#39;ll get our prediction $ tilde{Y}$ is by means of a weighted sum of each set of 3 inputs, and some nonlinear function $f$ which we call the &quot;activation function&quot; (or just &quot;activation&quot;). Pictorially, the process looks like the following, for each row $i$ of $X$ and $Y$, (where the columns of $X$ are shown arranged vertically instead of horizonally): . . In terms of matrix multiplication, since X is a 4x3 matrix, and Y is a 4x1 matrix, that implies that our weights should be a 3x1 matrix consisting of (unknown) values $w_0$, $w_1$ and $w_2$. The calculation can be written as: . $$ f left( overbrace{ left[ { begin{array}{ccc} 0 &amp; 0 &amp; 1 0 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 end{array} } right] }^ text{X} overbrace{ left[ { begin{array}{c} w_0 w_1 w_2 end{array} } right] }^{w} right) = overbrace{ left[ { begin{array}{c} 0 0 1 1 end{array} } right] }^{ tilde{Y}} $$Our nonlinear activation function $f$ is taken to operate on each row element one at a time, and we&#39;ll let $f_i$ denote the $i$th row of this completed activation, i.e.: . $$ f_i = f left( sum_j X_{ij}w_j right) = tilde{Y}_i . $$The particular activation function we will use is the &quot;sigmoid&quot;, . $$ f(x) = {1 over{1+e^{-x}}}, $$-- click here to see a plot of this function -- which has the derivative . $$ {df over dx} = {e^{-x} over(1 + e^{-x})^2} $$which can be shown (Hint: exercise for &quot;mathy&quot; students!) to simplify to $$ {df over dx}= f(1-f). $$ . The overall problem then amounts to finding the values of the &quot;weights&quot; $w_0, w_1,$ and $w_2$ so that the $ tilde{Y}$ we calculate is as close to the target $Y$ as possible. . To do this, we will seek to minimize a loss function defined as a sum across all data points we have, i.e. all 4 rows. The loss function $L$ we will choose is the mean square error loss, or MSE (note: later in Part 2 we will use a &#39;better&#39; loss function for this problem): . $$ L = {1 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]^2, $$or in terms of the activation function $$ L = {1 over N} sum_{i=0}^{N-1} left[ f_i - Y_i right]^2. $$ . Each of the weights $w_j$ ($j=0..2$) will start with random values, and then be updated via gradient descent, i.e. . $$ w_j^{new} = w_j^{old} - alpha{ partial L over partial w_j} $$where $ alpha$ is the learning rate, chosen to be some small parameter. For the MSE loss shown above, the partial derivatives with respect to each of the weights is . $$ { partial L over partial w_j} = {2 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]{ partial f_i over partial w_j} = {2 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]f_i(1-f_i)X_{ij}. $$Absorbing the factor of 2/N into our choice of $ alpha$, and writing the summation as a dot product, and noting that $f_i = tilde{Y}_i$, we can write the update for all the weights together as . $$ w = w - alpha X^T cdot left( [ tilde{Y}-Y]* tilde{Y}*(1- tilde{Y}) right) $$where the $ cdot$ denotes a matrix-matrix product (i.e. a dot product for successive rows of $X^T$) and $*$ denotes elementwise multiplication. . To clarify the above expression in terms of matrix dimensions, we can see that $w$, a 3x1 matrix, can be made by multipyting $X^T$ (a 3 x4 matrix) with the term in parentheses, i.e. the product of elementwise terms involving $ tilde{Y}$, which is a 4x1 matrix. In other words, a 3x4 matrix, times a 4x1 matrix, yields a 3x1 matrix. . Actual Code . The full code for all of this is then... . # Source: Slightly modified from Andrew Trask&#39;s code posted at # https://iamtrask.github.io/2015/07/12/basic-python-network/ import numpy as np # sigmoid activation def sigmoid(x,deriv=False): if(deriv==True): return x*(1-x) return 1/(1+np.exp(-x)) # input dataset X = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ]) # target output dataset Y = np.array([[0,0,1,1]]).T # seed random numbers to make calculation # deterministic (just a good practice) np.random.seed(1) # initialize weights randomly with mean 0 w = 2*np.random.random((3,1)) - 1 alpha = 1.0 # learning rate loss_history = [] # keep a record of how the loss proceeded, blank for now for iter in range(1000): # forward propagation Y_pred = sigmoid(np.dot(X,w)) # prediction, i.e. tilde{Y} # how much did we miss? diff = Y_pred - Y loss_history.append((diff**2).mean()) # add to the history of the loss # update weights w -= alpha * np.dot( X.T, diff*sigmoid(Y_pred, deriv=True)) print(&quot;Output After Training:&quot;) print(&quot;Y_pred = (should be two 0&#39;s followed by two 1&#39;s) n&quot;,Y_pred) print(&quot;weights = n&quot;,w) . Output After Training: Y_pred = (should be two 0&#39;s followed by two 1&#39;s) [[0.03178421] [0.02576499] [0.97906682] [0.97414645]] weights = [[ 7.26283009] [-0.21614618] [-3.41703015]] . Note that, because of our nonlinear activation, we don&#39;t get the solution $w_0=1, w_1=0, w_2=0$. . Plotting the loss vs. iteration number, we see... . %matplotlib inline import matplotlib.pyplot as plt plt.loglog(loss_history) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.show() . Change the activation function . Another popular choice of activation function is the rectified linear unit or ReLU. The function ReLU(x) is zero for x &lt;= 0, and equal to x (i.e. a straight line at 45 degrees for) x &gt;0. It can be written as max(x,0) or x * (x&gt;0), and its derivative is 1 for positive x, and zero otherwise. . Click here to see a graph of ReLU . Modifying our earlier code to use ReLU activation instead of sigmoid looks like this: . def relu(x,deriv=False): # relu activation if(deriv==True): return 1*(x&gt;0) return x*(x&gt;0) # seed random numbers to make calculation # deterministic (just a good practice) np.random.seed(1) # initialize weights randomly (but only &gt;0 because ReLU clips otherwise) w = np.random.random((3,1)) alpha = 0.3 # learning rate new_loss_history = [] # keep a record of how the error proceeded for iter in range(1000): # forward propagation Y_pred = relu(np.dot(X,w)) # how much did we miss? diff = Y_pred - Y new_loss_history.append((diff**2).mean()) # add to the record of the loss # update weights w -= alpha * np.dot( X.T, diff*relu(Y_pred, deriv=True)) print(&quot;Output After Training:&quot;) print(&quot;Y_pred = (should be two 0&#39;s followed by two 1&#39;s) n&quot;,Y_pred) print(&quot;weights = n&quot;,w) . Output After Training: Y_pred = (should be two 0&#39;s followed by two 1&#39;s) [[-0.] [-0.] [ 1.] [ 1.]] weights = [[ 1.01784368e+00] [ 8.53961786e-17] [-1.78436793e-02]] . # Aside/Trivia (you can skip this cell): I find it interesting that apparently w2 = 1-w0 print( w[2] - (1-w[0]) ) . [-3.46944695e-17] . Plot old results with new results: . %matplotlib inline import matplotlib.pyplot as plt plt.loglog(loss_history,label=&quot;sigmoid&quot;) plt.loglog(new_loss_history,label=&quot;relu&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend() plt.show() . Looks like ReLU may be a better choice than sigmoid for this problem! . Exercise: Read a 7-segment display . A 7-segment display is used for displaying numerical digits 0 through 9, usually by lighting up LEDs or parts of a liquid crystal display (LCD). The segments are labelled $a$ through $g$ according to the following diagram: . . Diagram of the network . The 7 inputs &quot;a&quot; through &quot;g&quot; will be mapped to 10 outputs for the individual digits, and each output can range from 0 (&quot;false&quot; or &quot;no&quot;) to 1 (&quot;true&quot; or &quot;yes&quot;) for that digit. The input and outputs will be connected by a matrix of weights. Pictorially, this looks like the following (Not shown: activation function $f$): . . ...where again, this network operates on a single data point at a time, datapoints which are rows of X and Y. What is shown in the above diagram are the columns of $X$ and $Y$ for a single row (/ single data point). . Create the dataset . Let the input X be the segments $a$ through $g$ are the columns of the input $X$, and are either 1 for on or 0 for off. Let the columns of the target $Y$ be the digits 0-9 themselves arranged in a &quot;one hot&quot; encoding scheme, as follows: . Digit | One-Hot Encoding for $Y$ | . 0 | 1,0,0,0,0,0,0,0,0,0 | . 1 | 0,1,0,0,0,0,0,0,0,0 | . 2 | 0,0,1,0,0,0,0,0,0,0 | . ... | ... | . 9 | 0,0,0,0,0,0,0,0,0,1 | . The values in the columns for $Y$ are essentially true/false &quot;bits&quot; for each digit, answering the question &quot;Is this digit the appropriate output?&quot; with a &quot;yes&quot;(=1) or &quot;no&quot; (=0) response. . The input $X$ will be a 10x7 matrix, and the target $Y$ will be a 10x10 matrix. Each row of $X$ will be the segments to produce the digit for that row. For example, the zeroth row of $X$ should show segments on which make an image of the digit zero, namely segments a, b, c, d, e, and f but not g, so that the zeroth row of X should be [1,1,1,1,1,1,0]. . Define numpy arrays for both $X$ and $Y$ (Hint: for $Y$, check out np.eye()): . # Students: fill these out completely for what the X and Y *should* be # for the 7-segment display. The following is just a &quot;stub&quot; to get you started. X = np.array([ [1,1,1,1,1,1,0], [], [], [] ]) Y = np.array([ [1,0,0,0,0,0,0,0,0,0], [], [] ]) . Initialize the weights . Previously the dimensions of the weight matrix $w$ were 3x1 because we were mapping each row of 3 elements in $X$ to each row of 1 element of $Y$. For this new problem, each row of $X$ has 7 elements, and we want to map those to the 10 elements in each 1-hot-encoded row of $Y$, so what should the dimensions of the weights matrix $w$ be? . Write some numpy code to randomly initialize the weights matrix: . np.random.seed(1) # initial RNG so everybody gets similar results w = np.random.random(( , )) # Students, fill in the array dimensions here . File &#34;&lt;ipython-input-7-20f53a51cded&gt;&#34;, line 1 w = np.random.random(( , )) ^ SyntaxError: invalid syntax . Train the network . Having created an $X$ and its matching $Y$, and initalized the weights $w$ randomly, train a neural network such as the ones above to learn to map a row of X to a row of Y, i.e. train it to recognize digits on 7-segment displays. Do this below. . # Students, copy training code from above and paste it here. # Use sigmoid activation, and 1000 iterations, and learning rate of 0.9 # Question: What happens if you use ReLU instead? Try it later. Is ReLU always the best choice? # And then print out your Y_pred &amp; weights matrix, and limit it to 3 significant digits print(&quot;Output After Training:&quot;) np.set_printoptions(formatter={&#39;float&#39;: lambda x: &quot;{0:0.3f}&quot;.format(x)}) # 3 sig figs print(&quot;Y_pred= n&quot;,Y_pred) print(&quot;weights = n&quot;,repr(w)) # the repr() makes it so it can be copied &amp; pasted back into Python code . Final Check: Keras version . Keras is a neural network library that lets us write NN applications very compactly. Try running the following using the X and Y from your 7-segment dataset: . # Just a demo of how one might similarly train a network using Keras import keras from keras.models import Sequential from keras.layers import Dense, Activation model = Sequential([ Dense(10, input_shape=(7,)), Activation(&#39;sigmoid&#39;) ]) model.compile(optimizer=&#39;adam&#39;, # We&#39;ll talk about optimizer choices and loss choices later loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(X, Y, epochs=200, batch_size=1) print(&quot; nY_tilde = n&quot;, model.predict(X) ) . Follow-up: Remarks . Re-stating what we just did . The original problem (posed at the top of this notebook) involves mapping some points from a 3-dimensional space into points in a 1-dimensional space, i.e. to points on the number line. The mapping is done by the combination of a weighted sum (a linear operation) and a nonlinear &quot;activation function&quot; applied to that sum. The use of an activation function like a sigmoid was originally intended to serve as an analogy of activation of biological neurons. Nonlinear activation functions are source of the &quot;power&quot; of neural networks (essentially we approximate some other function by means of a sum of basis functions in some function space, but don&#39;t worry about that if you&#39;re not math-inclined). The algorithm &#39;learns&#39; to approximate this operation via supervised learning and gradient descent according to some loss function. We used the mean squared error (MSE) for our loss, but lots and lots of different loss functions could be used, a few of which we&#39;ll look at another time. . Question for reflection: Unlike fitting a line $y = mx+b$, the weighted sum in our models in this notebook had no constant &quot;bias&quot; term like $b$. How might we include such a term? . One thing we glossed over: &quot;batch size&quot; . Question: Should we apply the gradient descent &quot;update&quot; to the weights each time we process a single row of $X$ &amp; $Y$, or should we compute the combined loss of all the rows together at the same time, and then do the update? This is essentially asking the same question as &quot;When fitting a line $mx+b$ to a bunch of data points, should we use all the points together to update $m$ and $b,$ or should we do this one point at a time -- compute the gradients of the loss at one point, update the weights, compute gradients at another point, etc.?&quot; . The number of points you use is called the batch size and it is what&#39;s known as a &quot;hyperparameter&quot; -- it is not part of the model per se, but it is a(n important) choice you make when training the model. The batch size affects the learning as follows: Averaging the gradints for many data points (i..e. a large batch size) will produce a smoother loss function and will also usually make the code execute more quickly through the dataset, but updating the weights for every point will cause the algorithm to learn with fewer iterations. . One quick way to observe this is to go up to the Keras code above and change batch_size from 1 to 10, and re-execute the cell. How is the accuracy after 200 iteractions, compared to when batch_size=1? . Terminology: Technically, it&#39;s called &quot;batch training&quot; when you sum the gradients for all the data points before updating the weights, whereas using fewer points is &quot;minibatch training&quot;, and updating for each point (i.e. each row, for us) is Stochastic Gradient Descent* (SGD -- more on these terms here). In practice, there is a tradeoff between smaller vs. larger (mini)batches, which has been the subject of intense scrutiny by researchers over the years. We will have more to say on this later. . For discussion later: In our presentation above, were we using batch training, minibatch training or SGD? . . . *Note: many people will regard SGD as an optimization algorithm per se, and refer to doing SGD even for (mini)batch sizes larger than 1. . Optional: If you want to go really crazy . How about training on this dataset: $$ overbrace{ left[ { begin{array}{cc} 0 &amp; 0 0 &amp; 1 1 &amp; 0 1 &amp; 1 end{array} } right] }^{X} rightarrow overbrace{ left[ { begin{array}{c} 0 1 1 0 end{array} } right] }^Y. $$ Good luck! ;-) (Hint 1: This problem features prominently in the history of Neural Networks, involving Marvin Minsky and &quot;AI Winter.&quot; Hint 2: This whole lesson could instead be entitled &quot;My First Artificial Neuron.&quot;) . Next time, we will go on to Part 2: Bias and CE Loss. . Additional Optional Exercise: Binary Math vs. One-Hot Encoding . For the 7-segment display, we used a one-hot encoding for our output, namely a set of true/false &quot;bits&quot; for each digit. One may wonder how effective this ouput-encoding method is, compared to a different bit-setting encoding method, namely binary representations. . Construct the target output matrix $Y$ for binary representations of the numbers 0 through 9. Your target matrix should have 10 rows and 4 columns (i.e, output bits for 1s, 2s, 4s, and 8s). | Using this $Y$ array, train the network as before, and plot the loss as a function of iteration. | Question: Which method works &#39;better&#39;? One-hot encoding or binary encoding? .",
            "url": "https://drscotthawley.github.io/blog/2019/01/30/My-First-Neural-Network.html",
            "relUrl": "/2019/01/30/My-First-Neural-Network.html",
            "date": " • Jan 30, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "Parallelizing Python Simplified",
            "content": "Parallelizing Python, Simplified . ![](/blog/images/parallelpython.png “”) . So you have some serial task that takes forever, and you’re thinking it should be parallelizable, but you find the documentation on this to be obtuse? Yea. . Usually I’m interested in either creating lots of data in parallel, or inputting lots of data in parallel, and it’s often something that I first implemented as a loop but got tired of how slow it runs. These involve embarrassingly parallel tasks in that they don’t depend on one another. . There’s a simple prescription for parallelizing most of these kinds of tasks in Python. It goes as follows: . Have some kind of task performed in a for loop. | Write a function that does what you want for one “instance.” For example, take what’s inside one of your for loops, put all that in a separate function. | As a check, keep your loop but use only the function call. Make sure it produces the same results as the original version of your code. | Use functools.partial to create a wrapper for your function. | Replace the loop with a call to Pool.map(). | In the following, we’ll cover 3 examples for parallel tasks: . Generate a bunch of files | Read a bunch of files into a list | Filling a numpy array | Example 1: Generate a bunch of files . Let’s say you have some important synthetic data that you want to generate lots of instances of. For now, for simplicity, we’re just going to generate images of, let’s say, random noise. And to make it interesting we’ll generate 2000 of them. . Here’s the serial for-loop version: . import numpy as np import cv2 n_images = 2000 size_x, size_y = 100, 100 for i in range(n_images): arr = 255*np.random.rand(size_x,size_y) filename = &#39;image_&#39;+str(i)+&#39;.png&#39; print(&quot;writing file &quot;,filename) cv2.imwrite(filename,arr) . Now we write a dedicated function, put it in a partial wrapper, and call it as follows: . import numpy as np import cv2 from functools import partial def write_one_file(size_x, size_y, name_prefix, i): arr = 255*np.random.rand(size_x,size_y) filename = name_prefix + str(i) + &#39;.png&#39; print(&quot;writing file &quot;,filename) cv2.imwrite(filename,arr) n_images = 2000 size_x, size_y = 100, 100 wrapper = partial(write_one_file, size_x, size_y, &#39;image_&#39;) for i in range(n_images): wrapper(i) . Finally we replace the loop with a multiprocessing pool. We can either use all the cpus on the machine (which is the default) or specify how many to use, by giving an argument to Pool(): . import numpy as np import cv2 from functools import partial import multiprocessing as mp def write_one_file(size_x, size_y, name_prefix, i): arr = 255*np.random.rand(size_x,size_y) filename = name_prefix + str(i) + &#39;.png&#39; print(&quot;writing file &quot;,filename) cv2.imwrite(filename,arr) n_images = 2000 size_x, size_y = 100, 100 wrapper = partial(write_one_file, size_x, size_y, &#39;image_&#39;) num_procs = mp.cpu_count() # or can replace with some number of processes to use pool = mp.Pool(num_procs) indices = range(n_images) results = pool.map(wrapper, indices) pool.close() pool.join() . There are other ways you can do this to get more control, e.g. to have each process in the pool receive a particular range of indices, but this basic setup will get the job done. And if you turn off the printing to screen and time the execution, you’ll see the speedup. . Example 2: Read a bunch of files into a list . This example is actually of limited utility and you may want to just skip down to “Example 3: Filling a numpy array,” but it’s still an illustrative example that motivates Example 3, and offers a bit of variety in how one might do things. In this case we’re not going to use Pool.map; instead we’re going to use a context manager for the particular datatype of list. . Let’s try to load in all the image files we just generated, into a list. Here’s the serial version: . import glob import cv2 name_prefix = &#39;image_&#39; # we&#39;ll use glob to get the list of available files # note that glob order isn&#39;t...easily discernible, so we&#39;ll sort. img_file_list = sorted(glob.glob(name_prefix+&#39;*.png&#39;)) n_files = len(img_file_list) print(n_files,&quot;files available.&quot;) img_data_list = [] for i in range(n_files): filename = name_prefix + str(i) + &#39;.png&#39; print(&quot;Reading file&quot;,filename) img = cv2.imread(filename) img_data_list.append(img) print(len(img_data_list),&quot;images in list.&quot;) . (If we wanted to, we could easily convert this list of images to a numpy array. But let’s hold off on that.) . This time, we’ll split up the tasks manually into equal numbers for each process. Parallelizing this can take the following form: . from multiprocessing import Process, Manager, cpu_count import glob import cv2 def load_one_proc(img_data_list, img_file_list, iproc, per_proc): istart, iend = iproc * per_proc, (iproc+1) * per_proc for i in range(istart,iend): # each process will read a range of files filename = img_file_list[i] print(&quot;Reading file&quot;,filename) img = cv2.imread(filename) img_data_list.append(img) return name_prefix = &#39;image_&#39; # we&#39;ll use glob to get the list of available files # note that glob order isn&#39;t...easily discernible, so we&#39;ll sort. img_file_list = sorted(glob.glob(name_prefix+&#39;*.png&#39;)) n_files = len(img_file_list) print(n_files,&quot;files available.&quot;) # We&#39;ll split up the list manually num_procs = cpu_count() print(&quot;Parallelizing across&quot;,num_procs,&quot;processes.&quot;) per_proc = n_files // num_procs # Number of files per processor to load assert n_files == per_proc * num_procs # Make sure tasks divide evenly. Obvously one can do something more sophisticated than this! with Manager() as manager: img_data_list = manager.list() processes = [] for iproc in range(num_procs): p = Process(target=load_one_proc, args=(img_data_list, img_file_list, iproc, per_proc)) p.start() processes.append(p) for p in processes: p.join() outside_list = list(img_data_list) # Copy out of the Manager context (there may be a better way to do this) print(len(outside_list),&quot;images in list.&quot;) . Okay, great. The thing is, that set of processes operates asynchronously, so there’s no telling what order the final list is going to be in. Maybe you don’t care, but sometimes I care. One way of dealing with this is to add an index item within the list for each item, and then sort on that index. . But most of the time what I really want in the end is a numpy array. So let’s just look at how to fill one of those, directly. . Example 3: Filling a NumPy array . Data scientist Jonas Teuwen made a great post which got me started on how to do this, but then it seems I uncovered a bug in numpy’s garbage collection for which there’s now a patch. Even without the patch, there are a couple workarounds one can use, and I’ll choose the simpler of the two workarounds. . Let’s load all those images into a numpy array instead of a list. First the serial version: . import numpy as np import glob import cv2 name_prefix = &#39;image_&#39; img_file_list = sorted(glob.glob(name_prefix+&#39;*.png&#39;)) n_files = len(img_file_list) first_image = cv2.imread(img_file_list[0]) print(n_files,&quot;files available. Shape of first image is&quot;,first_image.shape) print(&quot;Assuming all images are that size.&quot;) img_data_arr = np.zeros([n_files]+list(first_image.shape)) # allocate storage for i in range(n_files): filename = img_file_list[i] print(&quot;Reading file&quot;,filename) img_data_arr[i] = cv2.imread(filename) print(&quot;Finished.&quot;) . For the parallel version, we’re going to have to use a global variable. Sorry, there’s no away around it, because of Python’s Global Interpreter Lock (GIL). . Without further ado, here’s the parallel, numpy version of the ‘loading a list of images’ shown earlier in Example 2. (One other change: rather than specifying ranges of images for each processor – which I did just for the sake of variety – this time we’ll let Pool.map decide how to – evenly – distribute the tasks.) . import numpy as np import glob import cv2 from multiprocessing import Pool, sharedctypes, cpu_count from functools import partial import gc mp_shared_array = None # global variable for array def load_one_proc(img_file_list, i): global mp_shared_array # tmp will end up pointing to the memory address of the shared array we want to populate tmp = np.ctypeslib.as_array(mp_shared_array) filename = img_file_list[i] print(&quot;Reading file&quot;,filename) tmp[i] = cv2.imread(filename) # assign the values into the memory of the shared array return name_prefix = &#39;image_&#39; img_file_list = sorted(glob.glob(name_prefix+&#39;*.png&#39;)) n_files = len(img_file_list) first_image = cv2.imread(img_file_list[0]) print(n_files,&quot;files available. Shape of first image is&quot;,first_image.shape) print(&quot;Assuming all images are that size.&quot;) img_data_arr = np.zeros([n_files]+list(first_image.shape)) # allocate storage tmp = np.ctypeslib.as_ctypes(img_data_arr) # tmp variable avoids numpy garbage-collection bug print(&quot;Allocating shared storage for multiprocessing (this can take a while)&quot;) mp_shared_array = sharedctypes.RawArray(tmp._type_, tmp) num_procs = cpu_count() print(&quot;Parallelizing across&quot;,num_procs,&quot;processes.&quot;) p = Pool(num_procs) wrapper = partial(load_one_proc, img_file_list) indices = range(n_files) result = p.map(wrapper, indices) # here&#39;s where we farm out the op img_data_arr = np.ctypeslib.as_array(mp_shared_array, shape=img_data_arr.shape) # this actually happens pretty fast p.close() p.join() # Next couple lines are here just in case you want to move on to other things # and force garbage collection mp_shared_array = None gc.collect() print(&quot;Finished.&quot;) . So that’s the basic implementation. Let me know in the comments if you have suggestions for improvements, or other ideas! . P.S.- Final Remarks . Added in a couple thoughts, post-facto: . 1. Performance. Notice that what gets passed to p.map() is an iterator, which typically you’ll use as either the indices of the members of an array (as we did just now), or it as a range over the number of processors (kinda like we did in Example 2). In the former case, the system is likely to spawn lots and lots of processes (not all at the same time, but as one ‘job’ finishes the system will spawn a new one, and will keep the “pool” going), which will have a bit of overhead – i.e. latency – each time these start and stop. It’s not much though, and so you probably won’t notice if your goal is merely, “I’m doing this so that I only have to wait 5 minutes instead of an hour to get something done.” If instead you make the indices in the map over the range of processors on your machine and manually break up the array indices into chunks (sort of like we did in Example 2), then you won’t be spawning as many processes and so your latency will be considerably lower. But the gain may be small enough (depending on your system) that you may not notice the difference in performance. Still, if you want to go all-out for performance, then make the pool.map go over the number of procs you want. Otherwise, feel free to trust the system to do its thing for you and just use the array (or list) indices for the iterator. . 2. When Processes Die. Debugging multiprocessing runs is a pain. If one process dies (crashes, seg faults, generates any kind of “Error”), it will hang the entire pool and you won’t know why because the error messages won’t come to stdout or stderr. Look elsewhere for tutorials on tools for debugging multiprocessing runs. Good news is that regular print statements still come to stdout for all processes, so one way of debugging is the age-old method of just loading your code with print statements. .",
            "url": "https://drscotthawley.github.io/blog/2018/12/16/Parallelizing-Python-Simplified.html",
            "relUrl": "/2018/12/16/Parallelizing-Python-Simplified.html",
            "date": " • Dec 16, 2018"
        }
        
    
  
    
        ,"post10": {
            "title": "Resolving Mac OS X Aliases in Python",
            "content": "Mac OSX aliases are not symbolic links. Trying to read one may crash your code. . In an app I’m developing, I want users to be able to easily create a “library” of symbolic links to other places on their machine, and this is most easily achieved for many of them by Cmd-Option-Dragging and dropping the files. This creates an “alias”, which is a special file that Apple dreamed up. UNIX users are accustomed to symbolic links, and codes written in UNIX will not follow or “resolve” Mac aliases. Instead, they will cause an exception to be thrown. . There used to be some libraries to handle this, but they relied on Apple’s old Carbon framework which is no longer supported. There is a mac_alias package but the documentation is lacking. So, I found an old post on MacWorld where one solution is given, and I ported that for what I need. . Happy to share with you, so that you won’t have to worry about this. As an added bonus, you can tell it to convert aliases to symbolic links, so that “next time” you won’t have to deal with this. Enjoy. . #!/usr/bin/env python3 # # Resolve Mac OS X &#39;aliases&#39; by finding where they point to # Author: Scott H. Hawley # # Description: # Mac OSX aliases are not symbolic links. Trying to read one will probably crash your code. # Here a few routines to help. Run these to change the filename before trying to read a file. # Intended to be called from within other python code # # Python port modified from https://hints.macworld.com/article.php?story=20021024064107356 # # Requirements: osascript (AppleScript), platform, subprocess, shlex # # TODO: - could make it work in parallel when mutliple filenames are given # # NOTE: By default, this only returns the names of the original source files, # but if you set convert=True, it will also convert aliases to symbolic links. # import subprocess import platform import os # returns true if a file is an OSX alias, false otherwise def isAlias(path, already_checked_os=False): if (not already_checked_os) and (&#39;Darwin&#39; != platform.system()): # already_checked just saves a few microseconds ;-) return False checkpath = os.path.abspath(path) # osascript needs absolute paths # Next several lines are AppleScript line_1=&#39;tell application &quot;Finder&quot;&#39; line_2=&#39;set theItem to (POSIX file &quot;&#39;+checkpath+&#39;&quot;) as alias&#39; line_3=&#39;if the kind of theItem is &quot;alias&quot; then&#39; line_4=&#39; return true&#39; line_5=&#39;else&#39; line_6=&#39; return false&#39; line_7=&#39;end if&#39; line_8=&#39;end tell&#39; cmd = &quot;osascript -e &#39;&quot;+line_1+&quot;&#39; -e &#39;&quot;+line_2+&quot;&#39; -e &#39;&quot;+line_3+&quot;&#39; -e &#39;&quot;+line_4+&quot;&#39; -e &#39;&quot;+line_5+&quot;&#39; -e &#39;&quot;+line_6+&quot;&#39; -e &#39;&quot;+line_7+&quot;&#39; -e &#39;&quot;+line_8+&quot;&#39;&quot; args = shlex.split(cmd) # shlex splits cmd up appropriately so we can call subprocess.Popen with shell=False (better security) p = subprocess.Popen(args, shell=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) retval = p.wait() if (0 == retval): line = p.stdout.readlines()[0] line2 = line.decode(&#39;UTF-8&#39;).replace(&#39; n&#39;,&#39;&#39;) if (&#39;true&#39; == line2): return True else: return False else: print(&#39;resolve_osx_alias: Error: subprocess returned non-zero exit code &#39;+str(retval)) return None # returns the full path of the file &quot;pointed to&quot; by the alias def resolve_osx_alias(path, already_checked_os=False, convert=False): # single file/path name if (not already_checked_os) and (&#39;Darwin&#39; != platform.system()): # already_checked just saves a few microseconds ;-) return path checkpath = os.path.abspath(path) # osascript needs absolute paths # Next several lines are AppleScript line_1=&#39;tell application &quot;Finder&quot;&#39; line_2=&#39;set theItem to (POSIX file &quot;&#39;+checkpath+&#39;&quot;) as alias&#39; line_3=&#39;if the kind of theItem is &quot;alias&quot; then&#39; line_4=&#39; get the posix path of (original item of theItem as text)&#39; line_5=&#39;else&#39; line_6=&#39;return &quot;&#39;+checkpath+&#39;&quot;&#39; line_7 =&#39;end if&#39; line_8 =&#39;end tell&#39; cmd = &quot;osascript -e &#39;&quot;+line_1+&quot;&#39; -e &#39;&quot;+line_2+&quot;&#39; -e &#39;&quot;+line_3+&quot;&#39; -e &#39;&quot;+line_4+&quot;&#39; -e &#39;&quot;+line_5+&quot;&#39; -e &#39;&quot;+line_6+&quot;&#39; -e &#39;&quot;+line_7+&quot;&#39; -e &#39;&quot;+line_8+&quot;&#39;&quot; args = shlex.split(cmd) # shlex splits cmd up appropriately so we can call subprocess.Popen with shell=False (better security) p = subprocess.Popen(args, shell=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) retval = p.wait() if (0 == retval): line = p.stdout.readlines()[0] source = line.decode(&#39;UTF-8&#39;).replace(&#39; n&#39;,&#39;&#39;) if (convert): os.remove(checkpath) os.symlink(source, checkpath) else: print(&#39;resolve_osx_aliases: Error: subprocess returned non-zero exit code &#39;+str(retval)) source = &#39;&#39; return source # used for multiple files at a time, just a looped call to resolve_osx_alias def resolve_osx_aliases(filelist, convert=False): # multiple files #print(&quot;filelist = &quot;,filelist) if (&#39;Darwin&#39; != platform.system()): return filelist outlist = [] for infile in filelist: source = resolve_osx_alias(infile, already_checked_os=True, convert=convert) if (&#39;&#39; != source): outlist.append(source) #print(&quot;outlist = &quot;,outlist) return outlist if __name__ == &quot;__main__&quot;: import argparse parser = argparse.ArgumentParser(description=&#39;Resolve OSX aliases&#39;) parser.add_argument(&#39;file&#39;, help=&quot;alias files to resolve&quot;, nargs=&#39;+&#39;) args = parser.parse_args() outlist = resolve_osx_aliases(args.file) print(&quot;outlist = &quot;,outlist) . The above code is part of the utils/ directory in my Panotti package. The way it’s called is in the context of trying to read an audio file, called in the file panotti/datautils.py: . def load_audio(audio_path, mono=None, sr=None, convertOSXaliases=True): # wrapper for librosa.load try: signal, sr = librosa.load(audio_path, mono=mono, sr=sr) # try to read the file &#39;normally&#39; except NoBackendError as e: if (&#39;Darwin&#39; == platform.system()): # if an exception is thrown, check: Am I on a Mac? If so try to resolve an alias source = resolve_osx_alias(audio_path, convert=convertOSXaliases, already_checked_os=True) # ...and convert to symlinks for next time try: signal, sr = librosa.load(source, mono=mono, sr=sr) # Now try to read again except NoBackendError as e: # Ok, even that didn&#39;t work, giving up (for now). print(&quot; n*** ERROR: Could not open audio file {}&quot;.format(audio_path),&quot; n&quot;,flush=True) raise e else: # Failure for some other reason. print(&quot; n*** ERROR: Could not open audio file {}&quot;.format(audio_path),&quot; n&quot;,flush=True) raise e return signal, sr . Happy coding! . NOTE: Currently this code only follows _one_ alias. If there’s an alias pointing to an alias to a file, it won’t resolve to that file. Full generality would involve adding an iterative or recursive way of traversing multiple aliases which…I may do later. ;-) .",
            "url": "https://drscotthawley.github.io/blog/2018/02/21/Resolving-OSX-Aliases.html",
            "relUrl": "/2018/02/21/Resolving-OSX-Aliases.html",
            "date": " • Feb 21, 2018"
        }
        
    
  
    
        ,"post11": {
            "title": "Following Gravity - ML Foundations Part I",
            "content": "*Image credit: NASA* . Preface: I&#39;m writing this for myself, current students &amp; ASPIRE collaborators, and to &#39;give back&#39; to the internet community. I recently had insight into my &#39;main&#39; research problem, but started to hit a snag so decided to return to foundations. Going back to basics can be a good way to move forward... . By the end of this session, we will -- as an example problem -- have used the 1-dimensional path of an object in the presesece of gravity, to &#39;train&#39; a system to correctly infer (i.e. to &#39;learn&#39;) the constants of the motion -- initial position and velocity, and the acceleration due to gravity. Hopefully we learn a few other things along the way. ;-) . In the next installment, &quot;Part Ib,&quot; we&#39;ll derive the differential equation of motion, and in then in &quot;Part II&quot; we&#39;ll adapt the techniques we&#39;ve learned here to do signal processing. . Optimization Basics: Gradient Descent . Let&#39;s put the &quot;sample problem&quot; aside for now, and talk about the general problem of optimization. Often we may wish to minimize some function $f(x)$. In science, doing so may enable us to fit a curve to our data, as we&#39;ll do below. Similarly,&#39;machine learning&#39; systems often operate on the basis of minimizing a &#39;cost&#39; function to discern patterns in complex datasets. . Thus we want to find the value of $x$ for which $f(x)$ is the smallest. A graph of such a function might look like this... . (Python code follows, to make the graph) . import numpy as np, matplotlib.pyplot as plt fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;x&#39;, &#39;ylabel&#39;:&#39;f&#39;}) x = np.arange(-5,7,0.1) ax.plot(x,(x-1)**2+1) plt.show() . If $f(x)$ is differentiable and the derivative (i.e., slope) $df/dx$ can be evaluated easily, then we can perform a so-called &quot;gradient descent&quot;. . We do so as follows: . Start with some initial guess for $x$ | &quot;Go in the direction of $-df/dx$&quot;: $$x_{new} = x_{old} - alpha {df over dx},$$ where $ alpha$ is some parameter often called the &quot;learning rate&quot;. All this equation is saying is, &quot;If the function is increasing, then move to the left; and if the function is decreasing then move to the right.&quot; The actual change to $x$ is given by $ Delta x equiv - alpha (df/dx)$. | Repeat step 2 until some approximation criterion is met. | A nice feature of this method is that as $df/dx rightarrow 0$, so too $ Delta x rightarrow0$. So an &quot;adaptive stepsize&quot; is built-in. . Now let&#39;s try this out with some Python code... . from __future__ import print_function # for backwards-compatibility w/ Python2 import numpy as np, matplotlib.pyplot as plt def f(x): return (x-1)**2+1 def dfdx(x): return 2*(x-1) fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;x&#39;, &#39;ylabel&#39;:&#39;f&#39;}) x = np.arange(-5,7,0.1) ax.plot(x,f(x),ls=&#39;dashed&#39;) for alpha in ([0.002,0.1,0.25,0.8]): print(&quot;alpha = &quot;,alpha) x = -5 # starting point x_arr = [x] y_arr = [f(x)] maxiter = 50 for iter in range(maxiter): # do the descent # these two lines are just for plotting later x_arr.append(x) y_arr.append( f(x) ) # Here&#39;s the important part: update via gradient descent x = x - alpha * dfdx(x) # report and make the plot print(&quot; final x = &quot;,x) ax.plot(x_arr,y_arr,&#39;o-&#39;,label=&quot;alpha = &quot;+str(alpha)) handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels) plt.show() . alpha = 0.002 final x = -3.910414704056598 alpha = 0.1 final x = 0.9999143651384377 alpha = 0.25 final x = 0.9999999999999947 alpha = 0.8 final x = 0.999999999951503 . Notice how the larger learning rate ($ alpha$=0.8) meant that the steps taken were so large that they &quot;overshot&quot; the minimum, whereas the too-small learning rate ($ alpha=0.002$) still hadn&#39;t come anywhere close to the minimum before the maximum iteration was reached. . Exercise: Experiment by editing the above code: Try different learning rates and observe the behavior. . Challenge: Instability . You may have noticed, if you made the learning rate too large, that the algorithm does not converge to the solution but instead &#39;blows up&#39;. This is the &#39;flip side&#39; of the &#39;adaptive step size&#39; feature of this algorithm: If you jump &quot;across&quot; the minimum to the other side and end up a greater distance from the minimum that where you started, you will encounter an even larger gradient, which will lead to an even larger $ Delta x$, and so on. . We can see this with the same code from before, let&#39;s just use a different starting point and a step size that&#39;s clearly too large... . from __future__ import print_function # for backwards-compatibility w/ Python2 import numpy as np, matplotlib.pyplot as plt def f(x): return (x-1)**2+1 def dfdx(x): return 2*(x-1) alpha = 1.1 # &quot;too big&quot; learning rate print(&quot;alpha = &quot;,alpha) x = -1 # starting point x_arr = [] y_arr = [] maxiter = 12 for iter in range(maxiter): # do the descent x_arr.append(x) y_arr.append( f(x) ) x = x - alpha * dfdx(x) # report and make the plot print(&quot; final x = &quot;,x) fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;x&#39;, &#39;ylabel&#39;:&#39;f&#39;}) plt.plot(x_arr,y_arr,&#39;r&#39;,zorder=2,) plt.scatter(x_arr,y_arr,zorder=3,c=range(len(x_arr)),cmap=plt.cm.viridis) xlim = ax.get_xlim() # find out axis limits x = np.arange(xlim[0],xlim[1],1) # dashed line plt.plot(x,f(x),zorder=1,ls=&#39;dashed&#39;) handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels) plt.show() . alpha = 1.1 final x = -16.83220089651204 . In the above plot, we colored the points by iteration number, starting with the dark purple at the initial location of x=-1, and bouncing around ever-farther from the solution as the color changes to yellow. As this happens, the error is growing exponentially; this is one example of a numerical instability. Thus, this algorithm is not entirely stable. . . One way to guard against this to check: is our value of $f(x)$ at the current iteration larger than the value it was at the previous iteration? If so, that&#39;s a sign that our learning rate is too large, and we can use this criterion to dynamically adjust the learning rate. . Let&#39;s add some &#39;control&#39; code to that effect, to the previous script, and also print out the values of the relevant variables so we can track the progress: . from __future__ import print_function # for backwards-compatibility w/ Python2 import numpy as np, matplotlib.pyplot as plt def f(x): return (x-1)**2+1 def dfdx(x): return 2*(x-1) alpha = 13.0 # &quot;too big&quot; learning rate print(&quot;alpha = &quot;,alpha) x = -1 # starting point x_arr = [] y_arr = [] maxiter = 20 f_old = 1e99 # some big number for iter in range(maxiter): # do the descent # these two lines are just for plotting later x_arr.append(x) f_cur = f(x) y_arr.append( f_cur ) print(&quot;iter = &quot;,iter,&quot;x = &quot;,x,&quot;f(x) =&quot;,f(x),&quot;alpha = &quot;,alpha) if (f_cur &gt; f_old): # check for runaway behavior alpha = alpha * 0.5 print(&quot; decreasing alpha. new alpha = &quot;,alpha) f_old = f_cur # update via gradient descent x = x - alpha * dfdx(x) # report and make the plot print(&quot; final x = &quot;,x) fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;x&#39;, &#39;ylabel&#39;:&#39;f&#39;}) plt.plot(x_arr,y_arr,&#39;r&#39;,zorder=2,) plt.scatter(x_arr,y_arr,zorder=3,c=range(len(x_arr)),cmap=plt.cm.viridis) xlim = ax.get_xlim() x = np.arange(xlim[0],xlim[1],1) # x for dashed line plt.plot(x,f(x),zorder=1,ls=&#39;dashed&#39;) handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels) plt.show() . alpha = 13.0 iter = 0 x = -1 f(x) = 5 alpha = 13.0 iter = 1 x = 51.0 f(x) = 2501.0 alpha = 13.0 decreasing alpha. new alpha = 6.5 iter = 2 x = -599.0 f(x) = 360001.0 alpha = 6.5 decreasing alpha. new alpha = 3.25 iter = 3 x = 3301.0 f(x) = 10890001.0 alpha = 3.25 decreasing alpha. new alpha = 1.625 iter = 4 x = -7424.0 f(x) = 55130626.0 alpha = 1.625 decreasing alpha. new alpha = 0.8125 iter = 5 x = 4641.625 f(x) = 21535401.390625 alpha = 0.8125 iter = 6 x = -2899.390625 f(x) = 8412266.77758789 alpha = 0.8125 iter = 7 x = 1813.744140625 f(x) = 3286042.31937027 alpha = 0.8125 iter = 8 x = -1131.965087890625 f(x) = 1283610.8903790116 alpha = 0.8125 iter = 9 x = 709.1031799316406 f(x) = 501411.1134293014 alpha = 0.8125 iter = 10 x = -441.5644874572754 f(x) = 195864.32555832085 alpha = 0.8125 iter = 11 x = 277.6028046607971 f(x) = 76510.1115462191 alpha = 0.8125 iter = 12 x = -171.8767529129982 f(x) = 29887.37169774183 alpha = 0.8125 iter = 13 x = 109.04797057062387 f(x) = 11675.363944430403 alpha = 0.8125 iter = 14 x = -66.52998160663992 f(x) = 4561.298415793126 alpha = 0.8125 iter = 15 x = 43.20623850414995 f(x) = 1782.36656866919 alpha = 0.8125 iter = 16 x = -25.37889906509372 f(x) = 696.8463158864023 alpha = 0.8125 iter = 17 x = 17.486811915683575 f(x) = 272.8149671431259 alpha = 0.8125 iter = 18 x = -9.304257447302234 f(x) = 107.17772154028356 alpha = 0.8125 iter = 19 x = 7.440160904563896 f(x) = 42.47567247667326 alpha = 0.8125 final x = -3.025100565352435 . So in the preceding example, we start at $x=-1$, then the unstable behavior starts and we begin diverging from the minimum, so we decrease $ alpha$ as often as our criterion tells us to. Finally $ alpha$ becomes low enought to get the system &#39;under control&#39; and the algorithm enters the convergent regime. . Exercise: In the example above, we only decrease $ alpha$ by a factor of 2 each time, but it would be more efficient to decrease by a factor of 10. Try that and observe the behavior of the system. . You may say, &quot;Why do I need to worry about this instability stuff? As long as $ alpha&lt;1$ the system will converge, right?&quot; Well, for this simple system it seems obvious what needs to happen, but with multidimensional optimization problems (see below), it&#39;s not always obvious what to do. (Sometimes different &#39;dimensions&#39; need different learning rates.) This simple example serves as an introduction to phenomena which arise in more complex situations. . Challenge: Non-global minima . To explore more complicated functions, we&#39;re going to take advantage of the SymPy package, to let it take derivatives for us. Try executing the import in the next cell, and if nothing happens it means you have SymPy installed. If you get an error, you may need to go into a Terminal and run &quot;pip install sympy&quot;. . import sympy . You&#39;re good? No errors? Ok, moving on... . from __future__ import print_function # for backwards-compatibility w/ Python2 import numpy as np, matplotlib.pyplot as plt from sympy import Symbol, diff x = Symbol(&#39;x&#39;) # our function, more complicated (SymPy handles it!) f = (x-1)**4 - 20*(x-1)**2 + 10*x + 1 dfdx = diff(f,x) # setup fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;x&#39;, &#39;ylabel&#39;:&#39;f&#39;}) x_arr = np.arange(-5,7,0.1) y_arr = np.copy(x_arr) for i, val in enumerate(x_arr): y_arr[i] = f.evalf(subs={x:val}) ax.plot(x_arr,y_arr,ls=&#39;dashed&#39;) # space of &#39;error function&#39; # for a variety of learning rates... for alpha in ([0.002,0.01,0.03]): print(&quot;alpha = &quot;,alpha) xval = 6 # starting point x_arr = [xval] y_arr = [f.evalf(subs={x:xval})] maxiter = 50 # do the descent for iter in range(maxiter): # these two lines are just for plotting later x_arr.append(xval) y_arr.append( f.evalf(subs={x:xval}) ) # update via gradient descent xval = xval - alpha * dfdx.evalf(subs={x:xval}) print(&quot; final xval = &quot;,xval) ax.plot(x_arr,y_arr,&#39;o-&#39;,label=&quot;alpha = &quot;+str(alpha)) handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels) plt.show() . alpha = 0.002 final xval = 4.02939564594151 alpha = 0.01 final xval = 4.02896613891181 alpha = 0.03 final xval = -2.00328879556504 . All the runs start at $x=6$. Notice how the runs marked in organge and green go on to find a &quot;local&quot; minimum, but they don&#39;t find the &quot;global&quot; minimum (the overall lowest point) like the run marked in red does. The problem of ending up at non-global local minima is a generic problem for all kinds of optimization tasks. It tends to get even worse when you add more parameters... . Multidimensional Gradient Descent . (A descent into darkness...) . Let&#39;s define a function of two variables, that&#39;s got at least one minimum in it. We&#39;ll choose $$f(x,y) = - left( cos x + 3 cos y right) /2,$$ which actually has infinitely many minima, but we&#39;ll try to &#39;zoom in&#39; on just one. . We can vizualize this function via the graph produced by the code below; in the graph, darker areas show lower values than ligher areas, and there is a minimum at the point $x=0,y=0$ where $f(0,0)=-2$. . import numpy as np, matplotlib.pyplot as plt def f(x,y): return -( np.cos(x) + 3*np.cos(y) )/2 x = y = np.linspace(-4, 4, 100) z = np.zeros([len(x), len(y)]) for i in range(len(x)): for j in range(len(y)): z[j, i] = f(x[i], y[j]) fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;x&#39;, &#39;ylabel&#39;:&#39;y&#39;}) cs = ax.pcolor(x, y, z, cmap=plt.cm.afmhot) plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;box&#39;) cbar = fig.colorbar(cs, orientation=&#39;vertical&#39;) plt.show() . The way we find a minimum is similar to what we did before, except we use partial derivatives in the x- and y-directions: . $$x_{new} = x_{old} + Delta x, Delta x = - alpha { partial f over partial x} $$ $$y_{new} = y_{old} + Delta y, Delta y = - alpha { partial f over partial y},$$ . from __future__ import print_function # for backwards-compatibility w/ Python2 import numpy as np, matplotlib.pyplot as plt # our function def f(x,y): return -( np.cos(x) + 3*np.cos(y) )/2 def dfdx(x,y): return np.sin(x)/2 def dfdy(x,y): return 3*np.sin(y)/2 # variables for this run alpha = 0.5 xval, yval = 2.5, 1.5 # starting guess(es) x_arr = [] y_arr = [] maxiter = 20 for iter in range(maxiter): # gradient descent loop x_arr.append(xval) y_arr.append(yval) xval = xval - alpha * dfdx(xval,yval) yval = yval - alpha * dfdy(xval,yval) print(&quot;Final xval, yval = &quot;,xval,yval,&quot;. Target is (0,0)&quot;) # background image: plot the color background x = y = np.linspace(-4, 4, 100) z = np.zeros([len(x), len(y)]) for i in range(len(x)): for j in range(len(y)): z[j, i] = f(x[i], y[j]) fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;x&#39;, &#39;ylabel&#39;:&#39;y&#39;}) cs = ax.pcolor(x, y, z, cmap=plt.cm.afmhot) plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;box&#39;) cbar = fig.colorbar(cs, orientation=&#39;vertical&#39;) # plot the progress of our optimization plt.plot(x_arr,y_arr,zorder=1) plt.scatter(x_arr,y_arr,zorder=2,c=range(len(x_arr)),cmap=plt.cm.viridis) handles, labels = ax.get_legend_handles_labels() plt.show() . Final xval, yval = 0.0272555602238 3.59400699273e-12 . Target is (0,0) . In the above figure, we&#39;ve shown the &#39;path&#39; the algorithm takes in $x$-$y$ space, coloring the dots according to iteration number, so that the first points are dark purple, and later points tend to yellow. . Note that due to the asymmetry in the function (between $x$ and $y$), the path descends rapidly in $y$, and then travels along the &quot;valley&quot; in $x$ to reach the minimum. This &quot;long narrow valley&quot; behavior is common in multidimensional optimization problems: the system may &#39;solve&#39; one parameter quickly, but require thousands of operations to find the other one. . Many sophisticated schemes have arisen to handle this challenge, and we won&#39;t cover them here. For now, suffice it to say that, yes, this sort of thing happens. You may have &#39;found&#39; highly accurate values for certain parameters, but others are bogging down the process of convergence. . Next time, we&#39;ll cover a common application of optimization: Least Squares Regression... . Least Squares Regression . This is such a common thing to do in science and statistics, that everyone should learn how it works. We&#39;ll do it for linear relationships, but it generalizes to nonlinear situations as well. . How to Fit a Line . Let&#39;s say we&#39;re trying to fit a line to a bunch of data. We&#39;ve been given $n$ data points with coordinates $(x_i,y_i)$ where $i=1..n$. The problem becomes, given a line $f(x) = mx+b$, find the values of the parameters $m$ and $b$ which minimize the overall &quot;error&quot;. . add some kinda picture here? . The error can take many forms; one is the squared error $SE$, which is just the sum of the squares of the &quot;distances&quot; between each data point&#39;s $y$-value and the &quot;guess&quot; from the line fit $f$ at each value of $x$: . $$ SE = (f(x_1) - y_1)^2 + (f(x_2) - y_2)^2 + ... (f(x_n)-y_n)^2,$$ . We can write this concisely as $$ SE = sum_{i=1}^n (f(x_i)-y_i)^2.$$ . Another popular form is the &quot;mean squared error&quot; $MSE$, which is just $SE/n$: . $$ MSE = {1 over n} sum_{i=1}^n (f(x_i)-y_i)^2.$$ . The MSE has the nice feature that as you add more data points, it tends to hold a more-or-less consistent value (as opposed to the SE which gets bigger as you add more points). We&#39;ll use the MSE in the work that follows. . So expanding out $f(x)$, we see that the MSE is a function of $m$ and $b$, and these are the parameters we&#39;ll vary to minimize the MSE: $$ MSE(m,b) = {1 over n} sum_{i=1}^n (mx_i+b-y_i)^2.$$ . So, following our earlier work on multidimensional optimization, we start with guesses for $m$ and $b$ and then update according to gradient descent: . $$m_{new} = m_{old} + Delta m, Delta m = - alpha{ partial (MSE) over partial m} = - alpha{2 over n} sum_{i=1}^n (mx_i+b-y_i)(x_i) $$ $$b_{new} = b_{old} + Delta b, Delta b = - alpha{ partial (MSE) over partial b} = - alpha{2 over n} sum_{i=1}^n (mx_i+b-y_i)(1).$$ . So, to start off, let&#39;s get some data... . # Set up the input data n = 20 np.random.seed(1) # for reproducability x_data = np.random.uniform(size=n) # random points for x m_exact = 2.0 b_exact = 1.5 y_data = m_exact * x_data + b_exact y_data += 0.3*np.random.normal(size=n) # add noise # Plot the data def plot_data(x_data, y_data, axis_labels=(&#39;x&#39;,&#39;y&#39;), zero_y=False): fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:axis_labels[0], &#39;ylabel&#39;:axis_labels[1]}) ax.plot(x_data, y_data,&#39;o&#39;) if (zero_y): ax.set_ylim([0,ax.get_ylim()[1]*1.1]) plt.show() plot_data(x_data,y_data, zero_y=True) . Note: in contrast to earlier parts of this document which include complete python programs in every code post, for brevity&#39;s sake we will start using the notebook &quot;as intended&quot;, relying on the internal state and adding successive bits of code which make use of the &quot;memory&quot; of previously-defined variables. . Let&#39;s map out the MSE for this group of points, as a function of possible $m$ and $b$ values... . # map out the MSE for various values of m and b def MSE(x,y,m,b): # Use Python array operations to compute sums return ((m*x + b - y)**2).mean() mm = bb = np.linspace(0, 4, 50) z = np.zeros([len(mm), len(bb)]) for i in range(len(mm)): for j in range(len(bb)): z[j, i] = MSE(x_data,y_data, mm[i],bb[j]) fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;m&#39;, &#39;ylabel&#39;:&#39;b&#39;}) cs = ax.pcolor(mm, bb, np.log(z), cmap=plt.cm.afmhot) plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;box&#39;) plt.show() . We see the minimum near the &quot;exact&quot; values chosen in the begininng. (Note that we&#39;ve plotted the logarithm of the MSE just to make the colors stand out better.) . Next we will choose starting guesses for $m$ and $b$, and use gradient descent to fit the line... . m = 3.5 # initial guess b = 3.5 m_arr = [] b_arr = [] def dMSEdm(x,y,m,b): return (2*(m*x + b - y) *x).mean() def dMSEdb(x,y,m,b): return (2*(m*x + b - y)).mean() alpha = 0.1 maxiter, printevery = 500, 4 for iter in range(maxiter): m_arr.append(m) b_arr.append(b) if (0 == iter % printevery): print(iter,&quot;: b, m = &quot;,b,m,&quot;, MSE = &quot;,MSE(x_data,y_data,m,b)) m = m - alpha * dMSEdm(x_data,y_data,m,b) b = b - alpha * dMSEdb(x_data,y_data,m,b) print(&quot;Final result: m = &quot;,m,&quot;, b = &quot;,b) # background image: plot the color background (remembered from before) fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;m&#39;, &#39;ylabel&#39;:&#39;b&#39;}) cs = ax.pcolor(mm, bb, np.log(z), cmap=plt.cm.afmhot) plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;box&#39;) # plot the progress of our descent plt.plot(m_arr,b_arr,zorder=1) plt.scatter(m_arr,b_arr,zorder=2,c=range(len(m_arr)),cmap=plt.cm.viridis) handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels) plt.show() . 0 : b, m = 3.5 3.5 , MSE = 6.86780331186 4 : b, m = 2.07377614457 2.89890882764 , MSE = 0.98222306593 8 : b, m = 1.55966423863 2.66310750082 , MSE = 0.194874325956 12 : b, m = 1.37811928553 2.56128194633 , MSE = 0.0877947061277 16 : b, m = 1.31767685375 2.50899769214 , MSE = 0.0718728682069 20 : b, m = 1.30118421505 2.47541762467 , MSE = 0.0683627241086 24 : b, m = 1.30049838878 2.44926336309 , MSE = 0.0666665839706 28 : b, m = 1.30535957938 2.4263945595 , MSE = 0.0653286635918 32 : b, m = 1.3120331485 2.40527655618 , MSE = 0.0641346050201 36 : b, m = 1.31916510087 2.38532649645 , MSE = 0.0630441987699 40 : b, m = 1.32626977167 2.36630978168 , MSE = 0.0620438837066 44 : b, m = 1.33317800067 2.34811979986 , MSE = 0.0611251920416 48 : b, m = 1.33983578596 2.33069751134 , MSE = 0.0602811821983 52 : b, m = 1.34623082683 2.31400206963 , MSE = 0.059505695069 56 : b, m = 1.35236573256 2.29800006645 , MSE = 0.0587931380437 60 : b, m = 1.35824825911 2.28266157418 , MSE = 0.0581383942813 64 : b, m = 1.36388775858 2.26795866993 , MSE = 0.0575367695918 68 : b, m = 1.3692938954 2.2538648666 , MSE = 0.0569839532221 72 : b, m = 1.374476189 2.2403548761 , MSE = 0.0564759850457 76 : b, m = 1.37944385765 2.22740449497 , MSE = 0.0560092265247 80 : b, m = 1.3842057718 2.21499053585 , MSE = 0.0555803344137 84 : b, m = 1.38877044688 2.20309077677 , MSE = 0.0551862367311 88 : b, m = 1.39314605011 2.19168391801 , MSE = 0.0548241107275 92 : b, m = 1.39734041211 2.18074954278 , MSE = 0.0544913626571 96 : b, m = 1.4013610397 2.17026808021 , MSE = 0.054185609197 100 : b, m = 1.40521512901 2.16022077017 , MSE = 0.0539046603748 104 : b, m = 1.40890957817 2.1505896296 , MSE = 0.0536465038826 108 : b, m = 1.41245099963 2.14135742035 , MSE = 0.0534092906637 112 : b, m = 1.41584573193 2.1325076183 , MSE = 0.0531913216685 116 : b, m = 1.41909985108 2.12402438375 , MSE = 0.0529910356848 120 : b, m = 1.42221918141 2.11589253313 , MSE = 0.0528069981559 124 : b, m = 1.42520930601 2.10809751176 , MSE = 0.0526378909052 128 : b, m = 1.42807557671 2.10062536785 , MSE = 0.052482502695 132 : b, m = 1.43082312365 2.09346272749 , MSE = 0.0523397205507 136 : b, m = 1.4334568645 2.08659677074 , MSE = 0.0522085217891 140 : b, m = 1.43598151321 2.08001520867 , MSE = 0.0520879666937 144 : b, m = 1.43840158849 2.07370626138 , MSE = 0.0519771917836 148 : b, m = 1.44072142187 2.0676586369 , MSE = 0.0518754036286 152 : b, m = 1.44294516546 2.06186151097 , MSE = 0.051781873167 156 : b, m = 1.44507679941 2.0563045077 , MSE = 0.0516959304829 160 : b, m = 1.44712013898 2.05097768097 , MSE = 0.0516169600082 164 : b, m = 1.44907884142 2.04587149665 , MSE = 0.0515443961135 168 : b, m = 1.45095641248 2.04097681551 , MSE = 0.0514777190569 172 : b, m = 1.45275621269 2.03628487687 , MSE = 0.0514164512613 176 : b, m = 1.45448146341 2.03178728296 , MSE = 0.0513601538933 180 : b, m = 1.45613525255 2.0274759838 , MSE = 0.0513084237208 184 : b, m = 1.45772054011 2.0233432629 , MSE = 0.0512608902243 188 : b, m = 1.4592401635 2.01938172337 , MSE = 0.051217212943 192 : b, m = 1.4606968426 2.0155842747 , MSE = 0.0511770790366 196 : b, m = 1.46209318462 2.01194412009 , MSE = 0.0511402010444 200 : b, m = 1.46343168877 2.00845474426 , MSE = 0.0511063148261 204 : b, m = 1.46471475077 2.00510990182 , MSE = 0.0510751776703 208 : b, m = 1.46594466707 2.00190360604 , MSE = 0.0510465665558 212 : b, m = 1.46712363904 1.99883011819 , MSE = 0.0510202765543 216 : b, m = 1.46825377682 1.99588393722 , MSE = 0.0509961193626 220 : b, m = 1.46933710318 1.99305978998 , MSE = 0.0509739219537 224 : b, m = 1.4703755571 1.99035262169 , MSE = 0.0509535253378 228 : b, m = 1.47137099724 1.98775758697 , MSE = 0.0509347834233 232 : b, m = 1.47232520527 1.98527004115 , MSE = 0.0509175619703 236 : b, m = 1.47323988906 1.98288553192 , MSE = 0.0509017376297 240 : b, m = 1.47411668575 1.98059979141 , MSE = 0.0508871970587 244 : b, m = 1.47495716466 1.97840872852 , MSE = 0.0508738361101 248 : b, m = 1.4757628301 1.97630842161 , MSE = 0.0508615590853 252 : b, m = 1.47653512409 1.97429511148 , MSE = 0.0508502780498 256 : b, m = 1.47727542891 1.97236519463 , MSE = 0.0508399122026 260 : b, m = 1.47798506956 1.97051521684 , MSE = 0.050830387298 264 : b, m = 1.47866531621 1.96874186695 , MSE = 0.0508216351133 268 : b, m = 1.47931738637 1.96704197096 , MSE = 0.0508135929607 272 : b, m = 1.47994244714 1.96541248634 , MSE = 0.050806203238 276 : b, m = 1.48054161728 1.96385049656 , MSE = 0.0507994130159 280 : b, m = 1.4811159692 1.96235320595 , MSE = 0.0507931736592 284 : b, m = 1.4816665309 1.96091793458 , MSE = 0.0507874404782 288 : b, m = 1.4821942878 1.95954211357 , MSE = 0.0507821724088 292 : b, m = 1.48270018448 1.95822328041 , MSE = 0.0507773317183 296 : b, m = 1.48318512642 1.95695907462 , MSE = 0.0507728837349 300 : b, m = 1.4836499816 1.95574723348 , MSE = 0.0507687965998 304 : b, m = 1.48409558201 1.954585588 , MSE = 0.0507650410387 308 : b, m = 1.48452272522 1.95347205901 , MSE = 0.0507615901523 312 : b, m = 1.48493217573 1.95240465348 , MSE = 0.0507584192235 316 : b, m = 1.4853246664 1.95138146095 , MSE = 0.0507555055402 320 : b, m = 1.48570089973 1.95040065006 , MSE = 0.0507528282332 324 : b, m = 1.48606154909 1.94946046531 , MSE = 0.0507503681262 328 : b, m = 1.48640726001 1.94855922395 , MSE = 0.0507481075984 332 : b, m = 1.48673865124 1.94769531289 , MSE = 0.0507460304589 336 : b, m = 1.48705631592 1.94686718587 , MSE = 0.0507441218299 340 : b, m = 1.48736082261 1.9460733607 , MSE = 0.0507423680409 344 : b, m = 1.48765271634 1.94531241654 , MSE = 0.0507407565302 348 : b, m = 1.48793251954 1.94458299143 , MSE = 0.0507392757554 352 : b, m = 1.48820073302 1.94388377984 , MSE = 0.0507379151103 356 : b, m = 1.48845783683 1.94321353027 , MSE = 0.0507366648493 360 : b, m = 1.48870429115 1.9425710431 , MSE = 0.0507355160173 364 : b, m = 1.48894053709 1.94195516839 , MSE = 0.0507344603858 368 : b, m = 1.48916699749 1.9413648038 , MSE = 0.0507334903937 372 : b, m = 1.48938407768 1.94079889271 , MSE = 0.0507325990934 376 : b, m = 1.48959216619 1.9402564222 , MSE = 0.050731780101 380 : b, m = 1.48979163547 1.93973642136 , MSE = 0.0507310275504 384 : b, m = 1.48998284254 1.93923795946 , MSE = 0.0507303360514 388 : b, m = 1.49016612963 1.93876014435 , MSE = 0.0507297006512 392 : b, m = 1.49034182478 1.93830212081 , MSE = 0.0507291167986 396 : b, m = 1.49051024246 1.93786306905 , MSE = 0.0507285803118 400 : b, m = 1.49067168412 1.93744220325 , MSE = 0.0507280873482 404 : b, m = 1.49082643871 1.93703877012 , MSE = 0.0507276343768 408 : b, m = 1.49097478321 1.9366520476 , MSE = 0.0507272181534 412 : b, m = 1.49111698313 1.9362813435 , MSE = 0.0507268356966 416 : b, m = 1.491253293 1.93592599432 , MSE = 0.0507264842671 420 : b, m = 1.49138395677 1.93558536407 , MSE = 0.0507261613477 424 : b, m = 1.49150920832 1.93525884305 , MSE = 0.0507258646256 428 : b, m = 1.49162927183 1.93494584685 , MSE = 0.0507255919755 432 : b, m = 1.4917443622 1.93464581526 , MSE = 0.0507253414444 436 : b, m = 1.4918546854 1.93435821128 , MSE = 0.050725111238 440 : b, m = 1.49196043891 1.93408252014 , MSE = 0.0507248997074 444 : b, m = 1.49206181201 1.93381824839 , MSE = 0.0507247053375 448 : b, m = 1.49215898613 1.93356492304 , MSE = 0.0507245267361 452 : b, m = 1.4922521352 1.93332209068 , MSE = 0.0507243626239 456 : b, m = 1.49234142595 1.93308931667 , MSE = 0.0507242118256 460 : b, m = 1.49242701819 1.93286618439 , MSE = 0.0507240732609 464 : b, m = 1.49250906511 1.93265229447 , MSE = 0.0507239459375 468 : b, m = 1.49258771357 1.93244726408 , MSE = 0.0507238289434 472 : b, m = 1.49266310433 1.93225072625 , MSE = 0.0507237214405 476 : b, m = 1.49273537233 1.93206232921 , MSE = 0.050723622659 480 : b, m = 1.49280464692 1.93188173576 , MSE = 0.0507235318913 484 : b, m = 1.49287105209 1.93170862267 , MSE = 0.0507234484872 488 : b, m = 1.49293470669 1.9315426801 , MSE = 0.0507233718493 492 : b, m = 1.49299572466 1.93138361102 , MSE = 0.0507233014288 496 : b, m = 1.4930542152 1.93123113075 , MSE = 0.0507232367213 Final result: m = 1.93108496636 , b = 1.49311028301 . Note that the optimized values $(m,b)$ that we find may not exactly match the &quot;exact&quot; values we used to make the data, because the noise we added to the data can throw this off. In the limit where the noise amplitude goes to zero, our optimized values will exactly match the &quot;exact&quot; values used to generated the data. . Let&#39;s see the results of our line fit... . # plot the points fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;x&#39;, &#39;ylabel&#39;:&#39;y&#39;}) ax.plot(x_data,y_data,&#39;o&#39;) ax.set_ylim([0,ax.get_ylim()[1]*1.1]) # and plot the line we fit xlim = ax.get_xlim() x_line = np.linspace(xlim[0],xlim[1],2) y_line = m*x_line + b ax.plot(x_line,y_line) plt.show() . Great! . Least Squares Fitting with Nonlinear Functions . We can generalize the technique describe above to fit polynomials $$ f(x) = c_0 + c_1 x + c_2 x^2 + ...c_k x^k,$$ where $c_0...c_k$ are the parameters we will tune, and $k$ is the order of the polynomial. (Typically people use the letter $a$ for polynomial coefficients, but in the math rendering of Jupter, $ alpha$ and $a$ look too much alike, so we&#39;ll use $c$.) Written more succinctly, $$ f(x) = sum_{j=0}^k c_j x^j.$$ . (Indeed, we could even try non-polynomial basis functions, e.g., $ f(x) = c_0 + c_1 g(x) + c_2 h(x) + ...,$ but let&#39;s stick to polynomials for now.) . The key thing to note is that for each parameter $c_j$, the update $ Delta c_j$ will be . $$ Delta c_j = - alpha { partial (MSE) over partial c_j} = - alpha { partial (MSE) over partial f}{ partial f over partial c_j}$$ $$= - alpha {2 over n} sum_{i=1}^n [f(x_i)-y_i](x_i)^{j} $$ . (Note that we are not taking the derivative with respect to $x_i$, but rather with respect to $c_j$. Thus there is no &quot;power rule&quot; that needs be applied to this derivative. Also there is no sum over j.) . The following is a complete code for doing this, along with some added refinements: . $ alpha$ is now $ alpha_j$, i.e. different learning rates for different directions | we initialise $ alpha_j$ such that larger powers of $x$ start with smaller coefficients | we put the fitting code inside a method (with a bunch of parameters) so we can call it later | . from __future__ import print_function # for backwards-compatibility w/ Python2 import numpy as np, matplotlib.pyplot as plt def f(x,c): y = 0*x # f will work on single floats or arrays for j in range(c.size): y += c[j]*(x**j) return y def polyfit(x_data,y_data, c_start=None, order=None, maxiter=500, printevery = 25, alpha_start=0.9, alpha_start_power=0.3): # function definitions def MSE(x_arr,y_arr,c): f_arr = f(x_arr,c) return ((f_arr - y_arr)**2).mean() def dMSEdcj(x_arr,y_arr,c,j): # deriviative of MSE wrt cj (*not* wrt x!) f_arr = f(x_arr,c) return ( 2* ( f_arr - y_arr) * x_arr**j ).mean() if ((c_start is None) and (order is None)): print(&quot;Error: Either specify initial guesses for coefficients,&quot;, &quot;or specify the order of the polynomial&quot;) raise # halt if c_start is not None: order = c_start.size-1 c = np.copy(c_start) elif order is not None: c = np.random.uniform(size=order+1) # random guess for starting point assert(c.size == order+1) # check against conflicting info k = order print(&quot; Initial guess: c = &quot; ,np.array_str(c, precision=2)) alpha = np.ones(c.size) for j in range(c.size): # start with smaller alphas for higher powers of x alpha[j] = alpha_start*(alpha_start_power)**(j) MSE_old = 1e99 for iter in range(maxiter+1): # do the descent for j in range(c.size): c[j] = c[j] - alpha[j] * dMSEdcj(x_data,y_data,c,j) MSE_cur = MSE(x_data,y_data,c) if (MSE_cur &gt; MSE_old): # adjust if runaway behavior starts alpha[j] *= 0.3 print(&quot; Notice: decreasing alpha[&quot;,j,&quot;] to &quot;,alpha[j]) MSE_old = MSE_cur if (0 == iter % printevery): # progress log print(&#39;{:4d}&#39;.format(iter),&quot;/&quot;,maxiter,&quot;: MSE =&quot;,&#39;{:9.6g}&#39;.format(MSE_cur), &quot;, c = &quot;,np.array_str(c, precision=3),sep=&#39;&#39;) print(&quot;&quot;) return c # Set up input data n = 100 np.random.seed(2) # for reproducability x_data = np.random.uniform(-2.5,3,size=n) # some random points for x c_data = np.array([-4,-3,5,.5,-2,.5]) # params to generate data (5th-degree polynomial) y_data = f(x_data, c_data) y_data += 0.02*np.random.normal(size=n)*y_data # add a (tiny) bit of noise #- Perform Least Squares Fit c = polyfit(x_data, y_data, c_start=c_data*np.random.random(), maxiter=500) #-- Plot the results def plot_data_and_curve(x_data,y_data,axis_labels=(&#39;x&#39;,&#39;y&#39;), ): # plot the points fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:axis_labels[0], &#39;ylabel&#39;:axis_labels[1]}) ax.plot(x_data,y_data,&#39;o&#39;) # and plot the curve we fit xlim = ax.get_xlim() x_line = np.linspace(xlim[0],xlim[1],100) y_line = f(x_line, c) ax.plot(x_line,y_line) plt.show() plot_data_and_curve(x_data,y_data) . Initial guess: c = [-3.52 -2.64 4.4 0.44 -1.76 0.44] Notice: decreasing alpha[ 3 ] to 0.00729 Notice: decreasing alpha[ 4 ] to 0.002187 Notice: decreasing alpha[ 5 ] to 0.0006561 0/500: MSE = 258.233, c = [-5.438 -1.633 4.24 0.555 -1.904 0.765] Notice: decreasing alpha[ 5 ] to 0.00019683 25/500: MSE = 0.529541, c = [-4.265 -1.545 5.668 -0.392 -2.146 0.612] 50/500: MSE = 0.424417, c = [-4.304 -1.808 5.659 -0.241 -2.137 0.595] 75/500: MSE = 0.335586, c = [-4.256 -2.034 5.552 -0.105 -2.115 0.578] 100/500: MSE = 0.275848, c = [-4.212 -2.218 5.457 0.006 -2.096 0.564] 125/500: MSE = 0.236521, c = [-4.175 -2.367 5.38 0.096 -2.08 0.553] 150/500: MSE = 0.21068, c = [-4.146 -2.488 5.317 0.17 -2.068 0.544] 175/500: MSE = 0.193702, c = [-4.122 -2.586 5.267 0.229 -2.058 0.537] 200/500: MSE = 0.182549, c = [-4.103 -2.665 5.226 0.277 -2.049 0.531] 225/500: MSE = 0.175222, c = [-4.087 -2.73 5.192 0.316 -2.042 0.526] 250/500: MSE = 0.170408, c = [-4.075 -2.782 5.165 0.347 -2.037 0.522] 275/500: MSE = 0.167245, c = [-4.064 -2.824 5.143 0.373 -2.033 0.519] 300/500: MSE = 0.165167, c = [-4.056 -2.859 5.126 0.393 -2.029 0.516] 325/500: MSE = 0.163802, c = [-4.049 -2.886 5.111 0.41 -2.026 0.514] 350/500: MSE = 0.162905, c = [-4.044 -2.909 5.1 0.424 -2.024 0.513] 375/500: MSE = 0.162316, c = [-4.039 -2.927 5.09 0.435 -2.022 0.511] 400/500: MSE = 0.161929, c = [-4.036 -2.942 5.083 0.444 -2.02 0.51 ] 425/500: MSE = 0.161675, c = [-4.033 -2.954 5.076 0.451 -2.019 0.509] 450/500: MSE = 0.161508, c = [-4.031 -2.964 5.071 0.457 -2.018 0.508] 475/500: MSE = 0.161398, c = [-4.029 -2.972 5.067 0.462 -2.017 0.508] 500/500: MSE = 0.161326, c = [-4.027 -2.978 5.064 0.465 -2.017 0.507] . Now, it turns out that polynomials are often terrible things to try to fit arbitrary data with, because they can &#39;blow up&#39; as $|x|$ increases, and this causes instability. But for a variety of physics problems (see below), polynomials can be just what we&#39;re after. Plus, that made a nice demonstration, for now. . (For more general functions, I actually wrote a multi-parameter SymPy gradient-descient that is completely general, but it&#39;s terrifically slow so I won&#39;t be posting it here. If you really want it, contact me.) . Learning Gravity . Ok. Now we&#39;re all we&#39;re going to do next is fit a parabola to the motion of a falling ball -- and that&#39;s supposed to tell us something deep about physics. Sounds silly, right? &#39;Everybody&#39; knows objects moving in a gravitational field follow parabolas (both in space &amp; time); the more math-savvy may complain that we&#39;re simply going to &#39;get out of this&#39; what we &#39;put into it.&#39; . Well, from a philosophical standpoint and from the way that these methods will generalize to other situations, there are significant implications from the methodology we&#39;re about to follow. . The Challenge: Given a set of one-dimensional data of position vs. time $y(t)$, can we find the underlying equation that gives rise to it? Better put, can we fit a model to it, and how well can we fit it, and what kind of model will it be anyway? . (This is the sort of thing that statisticians do, but it&#39;s also something physicists do, and one could argue, this is what everybody does all the time. ) . Let&#39;s get started. I&#39;m just going to specify y(t) at a series of $n+1$ time steps $t_i$ ($t_0$...$t_n$) and we&#39;ll make them evenly spaced, and we&#39;ll leave out any noise at all -- perfect data. :-) . g_exact = 9.8 # a physical parater we&#39;ll find a fit for dt = 0.01 tmax = 1 # number of time steps t_data = np.arange(0,tmax,step=dt) # time values nt = t_data.size print(&quot;dt = &quot;,dt,&quot;, nt = &quot;,nt) y0 = 1.234 # initial position, choose anything v0 = 3.1415 # initial velocity #assign the data y_data = y0 + v0*t_data - 0.5 * g_exact * t_data**2 # y_data *= np.random.uniform(low=.9, high=1.1, size=(y_data.size)) # for later; add noise in plot_data(t_data,y_data, axis_labels=(&#39;t&#39;,&#39;y&#39;)) . dt = 0.01 , nt = 100 . Can we fit this with a polynomial? Sure, let&#39;s do that, using the code from before... . c = polyfit(t_data, y_data, order=2, alpha_start = 10.0, maxiter=1000, printevery=100) print(&quot;Our fit: y(t) = &quot;,c[0],&quot; + &quot;,c[1],&quot;*t + &quot;,c[2],&quot;*t**2&quot;,sep=&#39;&#39;) print(&quot;Compare to exact: y(t) = &quot;,y0, &quot; + &quot;,v0, &quot;*t - &quot;,0.5*g_exact,&quot;*t**2&quot;,sep=&#39;&#39;) print(&quot;Estimate for g = &quot;,-2*c[2]) plot_data_and_curve(t_data,y_data, axis_labels=(&#39;t&#39;,&#39;y&#39;)) . Initial guess: c = [ 0.72 0.71 0.77] 0/1000: MSE = 5.41899, c = [-2.186 7.319 -1.042] Notice: decreasing alpha[ 0 ] to 3.0 Notice: decreasing alpha[ 0 ] to 0.9 100/1000: MSE =0.0314071, c = [ 1.642 0.749 -2.528] 200/1000: MSE =0.00280409, c = [ 1.356 2.427 -4.191] 300/1000: MSE =0.000250355, c = [ 1.27 2.928 -4.688] 400/1000: MSE =2.23522e-05, c = [ 1.245 3.078 -4.837] 500/1000: MSE =1.99565e-06, c = [ 1.237 3.122 -4.881] 600/1000: MSE =1.78176e-07, c = [ 1.235 3.136 -4.894] 700/1000: MSE =1.59079e-08, c = [ 1.234 3.14 -4.898] 800/1000: MSE =1.42029e-09, c = [ 1.234 3.141 -4.899] 900/1000: MSE =1.26806e-10, c = [ 1.234 3.141 -4.9 ] 1000/1000: MSE =1.13215e-11, c = [ 1.234 3.141 -4.9 ] Our fit: y(t) = 1.23400775143 + 3.14145457517*t + -4.89995497009*t**2 Compare to exact: y(t) = 1.234 + 3.1415*t - 4.9*t**2 Estimate for g = 9.79990994018 . What if we try fitting higher-order terms? Are their coefficients negligible? The system may converge, but it will take a lot more iterations... (be prepared to wait!) . c = polyfit(t_data, y_data, order=3, alpha_start = 1.0, maxiter=700000, printevery=10000) print(&quot;Our fit: y(t) = &quot;,c[0],&quot; + &quot;,c[1],&quot;*t + &quot;,c[2],&quot;*t**2 + &quot;,c[3],&quot;*t**3&quot;,sep=&#39;&#39;) print(&quot;Compare to exact: y(t) = &quot;,y0, &quot; + &quot;,v0, &quot;*t - &quot;,0.5*g_exact,&quot;*t**2&quot;,sep=&#39;&#39;) print(&quot;Estimate for g = &quot;,-2*c[2]) . Initial guess: c = [ 0.33 0.23 0.63 0.41] 0/700000: MSE = 0.828106, c = [ 1.189 -0.045 0.563 0.398] Notice: decreasing alpha[ 0 ] to 0.3 10000/700000: MSE =0.000464818, c = [ 1.291 2.454 -3.188 -1.138] 20000/700000: MSE =0.000369748, c = [ 1.285 2.528 -3.373 -1.015] 30000/700000: MSE =0.000294122, c = [ 1.279 2.594 -3.538 -0.906] 40000/700000: MSE =0.000233965, c = [ 1.275 2.654 -3.685 -0.808] 50000/700000: MSE =0.000186111, c = [ 1.27 2.706 -3.817 -0.72 ] 60000/700000: MSE =0.000148045, c = [ 1.266 2.753 -3.934 -0.642] 70000/700000: MSE =0.000117765, c = [ 1.263 2.795 -4.038 -0.573] 80000/700000: MSE =9.36783e-05, c = [ 1.26 2.833 -4.131 -0.511] 90000/700000: MSE =7.4518e-05, c = [ 1.257 2.866 -4.214 -0.456] 100000/700000: MSE =5.92766e-05, c = [ 1.254 2.896 -4.289 -0.407] 110000/700000: MSE =4.71526e-05, c = [ 1.252 2.922 -4.355 -0.363] 120000/700000: MSE =3.75083e-05, c = [ 1.25 2.946 -4.414 -0.323] 130000/700000: MSE =2.98366e-05, c = [ 1.248 2.967 -4.466 -0.288] 140000/700000: MSE =2.37341e-05, c = [ 1.247 2.986 -4.513 -0.257] 150000/700000: MSE =1.88797e-05, c = [ 1.246 3.003 -4.555 -0.229] 160000/700000: MSE =1.50182e-05, c = [ 1.244 3.018 -4.592 -0.205] 170000/700000: MSE =1.19465e-05, c = [ 1.243 3.031 -4.626 -0.183] 180000/700000: MSE =9.50301e-06, c = [ 1.242 3.043 -4.655 -0.163] 190000/700000: MSE =7.55933e-06, c = [ 1.241 3.054 -4.682 -0.145] 200000/700000: MSE =6.0132e-06, c = [ 1.24 3.063 -4.705 -0.129] 210000/700000: MSE =4.7833e-06, c = [ 1.24 3.072 -4.726 -0.115] 220000/700000: MSE =3.80496e-06, c = [ 1.239 3.079 -4.745 -0.103] 230000/700000: MSE =3.02672e-06, c = [ 1.239 3.086 -4.762 -0.092] 240000/700000: MSE =2.40766e-06, c = [ 1.238 3.092 -4.777 -0.082] 250000/700000: MSE =1.91521e-06, c = [ 1.238 3.097 -4.79 -0.073] 260000/700000: MSE =1.52349e-06, c = [ 1.237 3.102 -4.802 -0.065] 270000/700000: MSE =1.21188e-06, c = [ 1.237 3.106 -4.813 -0.058] 280000/700000: MSE =9.64014e-07, c = [ 1.237 3.11 -4.822 -0.052] 290000/700000: MSE =7.66841e-07, c = [ 1.236 3.114 -4.83 -0.046] 300000/700000: MSE =6.09997e-07, c = [ 1.236 3.117 -4.838 -0.041] 310000/700000: MSE =4.85233e-07, c = [ 1.236 3.119 -4.845 -0.037] 320000/700000: MSE =3.85987e-07, c = [ 1.236 3.122 -4.851 -0.033] 330000/700000: MSE =3.0704e-07, c = [ 1.235 3.124 -4.856 -0.029] 340000/700000: MSE =2.4424e-07, c = [ 1.235 3.126 -4.861 -0.026] 350000/700000: MSE =1.94285e-07, c = [ 1.235 3.127 -4.865 -0.023] 360000/700000: MSE =1.54547e-07, c = [ 1.235 3.129 -4.869 -0.021] 370000/700000: MSE =1.22937e-07, c = [ 1.235 3.13 -4.872 -0.019] 380000/700000: MSE =9.77925e-08, c = [ 1.235 3.132 -4.875 -0.017] 390000/700000: MSE =7.77907e-08, c = [ 1.235 3.133 -4.878 -0.015] 400000/700000: MSE =6.188e-08, c = [ 1.235 3.134 -4.88 -0.013] 410000/700000: MSE =4.92235e-08, c = [ 1.235 3.134 -4.882 -0.012] 420000/700000: MSE =3.91556e-08, c = [ 1.235 3.135 -4.884 -0.01 ] 430000/700000: MSE =3.1147e-08, c = [ 1.234 3.136 -4.886 -0.009] 440000/700000: MSE =2.47764e-08, c = [ 1.234 3.136 -4.887 -0.008] 450000/700000: MSE =1.97088e-08, c = [ 1.234 3.137 -4.889 -0.007] 460000/700000: MSE =1.56777e-08, c = [ 1.234 3.138 -4.89 -0.007] 470000/700000: MSE =1.24711e-08, c = [ 1.234 3.138 -4.891 -0.006] 480000/700000: MSE =9.92037e-09, c = [ 1.234 3.138 -4.892 -0.005] 490000/700000: MSE =7.89133e-09, c = [ 1.234e+00 3.139e+00 -4.893e+00 -4.691e-03] 500000/700000: MSE =6.27729e-09, c = [ 1.234e+00 3.139e+00 -4.894e+00 -4.184e-03] 510000/700000: MSE =4.99338e-09, c = [ 1.234e+00 3.139e+00 -4.894e+00 -3.731e-03] 520000/700000: MSE =3.97207e-09, c = [ 1.234e+00 3.139e+00 -4.895e+00 -3.328e-03] 530000/700000: MSE =3.15965e-09, c = [ 1.234e+00 3.140e+00 -4.896e+00 -2.968e-03] 540000/700000: MSE =2.5134e-09, c = [ 1.234e+00 3.140e+00 -4.896e+00 -2.647e-03] 550000/700000: MSE =1.99932e-09, c = [ 1.234e+00 3.140e+00 -4.896e+00 -2.361e-03] 560000/700000: MSE =1.5904e-09, c = [ 1.234e+00 3.140e+00 -4.897e+00 -2.106e-03] 570000/700000: MSE =1.26511e-09, c = [ 1.234e+00 3.140e+00 -4.897e+00 -1.878e-03] 580000/700000: MSE =1.00635e-09, c = [ 1.234e+00 3.140e+00 -4.897e+00 -1.675e-03] 590000/700000: MSE =8.0052e-10, c = [ 1.234e+00 3.141e+00 -4.898e+00 -1.494e-03] 600000/700000: MSE =6.36787e-10, c = [ 1.234e+00 3.141e+00 -4.898e+00 -1.332e-03] 610000/700000: MSE =5.06543e-10, c = [ 1.234e+00 3.141e+00 -4.898e+00 -1.188e-03] 620000/700000: MSE =4.02939e-10, c = [ 1.234e+00 3.141e+00 -4.898e+00 -1.060e-03] 630000/700000: MSE =3.20524e-10, c = [ 1.234e+00 3.141e+00 -4.899e+00 -9.454e-04] 640000/700000: MSE =2.54967e-10, c = [ 1.234e+00 3.141e+00 -4.899e+00 -8.432e-04] 650000/700000: MSE =2.02818e-10, c = [ 1.234e+00 3.141e+00 -4.899e+00 -7.520e-04] 660000/700000: MSE =1.61335e-10, c = [ 1.234e+00 3.141e+00 -4.899e+00 -6.707e-04] 670000/700000: MSE =1.28336e-10, c = [ 1.234e+00 3.141e+00 -4.899e+00 -5.982e-04] 680000/700000: MSE =1.02087e-10, c = [ 1.234e+00 3.141e+00 -4.899e+00 -5.335e-04] 690000/700000: MSE =8.12072e-11, c = [ 1.234e+00 3.141e+00 -4.899e+00 -4.758e-04] 700000/700000: MSE =6.45976e-11, c = [ 1.234e+00 3.141e+00 -4.899e+00 -4.244e-04] Our fit: y(t) = 1.23402130221 + 3.1412436463*t + -4.89936171114*t**2 + -0.000424401050714*t**3 Compare to exact: y(t) = 1.234 + 3.1415*t - 4.9*t**2 Estimate for g = 9.79872342227 . So, in this case, we were able to show not only that the data fits a parabola well, but that the higher order term (for $t^3$) is negigible!! Great science! In practice, however, for non-perfect data, this does not work out. The higher-order term introduces an extreme sensitivity to the noise, which can render the results inconclusive. . Exercise: Go back to where the data is generated, and uncomment the line that says &quot;# for later; add noise in&quot; and re-run the fitting. You will find that the coefficients for the cubic polynomial do not resemble the original values found at all, whereas the coefficients for a quadratic polynomial, while not being the same as before, will still be &quot;close.&quot; . Thus, by hypothesizing a parabolic dependence, we&#39;re able to correctly deduce the parameters of the motion (initial position &amp; velocity, and acceleration), and we get a very low error in doing so. :-) Trying to show that higher-order terms in a polynomial expansion don&#39;t contribute...that worked for &quot;perfect data&quot; but in a practical case it didn&#39;t work out because polynomials are &quot;ill behaved.&quot; Still, we got some useful physics out of it. And that works for many applications. We could stop here. . ...although... . What if our data wasn&#39;t parabolic? Sure, for motion in a uniform gravitational field this is fine, but what if we want to model the sinusoidal motion of a simple harmonic oscillator? In that case, guessing a parabola would only work for very early times (thanks to Taylor&#39;s theorem). Sure, we could fit a model where we&#39;ve explictly put in a sine function in the code -- and I encourage you to write your own code to do this -- but perhaps there&#39;s a way to deduce the motion, by looking at the local behavior and thereby &#39;learning&#39; the differential equation underlying the motion. . Exercise: Copy the polyfit() code elsewhere (e.g. to text file or a new cell in this Jupyter notebook or a new notebook) and rename it sinefit(), and modify it to fit a sine function instead of a polynomial: . $$y(t) = A sin( omega t + phi),$$ . where the fit parameters will be the amplitude $A$, frequency $ omega$ and phase constant $ phi$. Try fitting to data generated for $A=3$, $ omega=2$, $ phi=1.57$ on $0 le t le 10$. As an example, you can check your answer against this. . The discussion goes on, but I&#39;m breaking it off into a &quot;Part Ib&quot; for a separate post. In that post, we&#39;ll switch from fitting the data &quot;globally&quot; to looking &quot;locally,&quot; in preparation for work in &quot;Time Series Prediction.&quot; -SH . . Afterward: Alternatives to &quot;Simple&quot; Gradient Descent . There are lots of schemes that incorporate more sophisticated approaches in order to achieve convergence more reliabily and more quickly than the &quot;simple&quot; gradient descent we&#39;ve been doing. . Such schemes introduce concepts such as &quot;momentum&quot; and go by names such as Adagrad, Adadelta, Adam, RMSProp, etc... For an excellent overview of such methods, I recommend Sebastian Ruder&#39;s blog post which includes some great animations! .",
            "url": "https://drscotthawley.github.io/blog/2017/02/23/Following-Gravity-Colab.html",
            "relUrl": "/2017/02/23/Following-Gravity-Colab.html",
            "date": " • Feb 23, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://drscotthawley.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://drscotthawley.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}