{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LATEST Notes on Convexity \n",
    "\n",
    "> Investigating convexity for simple neural networks\n",
    "- title: or is this the title\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- hide: false\n",
    "- comments: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2\n",
    "- description:  Following up on a question that arose in Week 3 of Andrew Ng's Machine Learning course. \n",
    "- image: images/convexity/convexity_header_new.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a post where I'm investigating [convexity](https://en.wikipedia.org/wiki/Convex_function) a bit, as it relates to neural networks.\n",
    "\n",
    "Andrew Ng, in Week 3 of his [Coursera course on Machine Learning](https://www.coursera.org/learn/machine-learning/home/welcome), shows the following image with respect to the \"cost function\" to be optimized ([this is slide 14 of Lecture 6](https://d18ky98rnyall9.cloudfront.net/_964b8d77dc0ee6fd42ac7d8a70c4ffa1_Lecture6.pdf?Expires=1511481600&Signature=axhpATyHYVHcJnPtXUFN9~PGYBz~RTLyUrApDUaUlEWS19gg3fDjtfQeez45m6b9AQAlcYw0MvLd3sitwkdfwzofJBJ4SwIXnss4nWt-CtiuHCdbYMGz6jjenc0KOPTlDUMcGNvsuDV8SEH1wTRV76pO94IeF85~SyalebsTGFA_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)):\n",
    "\n",
    "\n",
    "![ng_convexity](images/convexity/ng_loss_convexity.png)\n",
    "\n",
    "I wanted to investigate this: Could I reproduce the two graphs he sketched?   The two different loss functions are the mean squared error (MSE or sometimes just SE) and cross entropy (CE):\n",
    "\n",
    "$$ MSE = {1\\over m}\\sum_{i=1}^m \\left( y_i - h_i \\right)^2 $$\n",
    "\n",
    "$$ CE =  - {1\\over m}\\sum_{i=1}^m \\left[ y_i \\log(h_i) + (1-y_i) \\log(1-h_i) \\right] $$\n",
    "\n",
    "where $y_i$ are the true values (0 or 1) and $h_i = h(x_i)$ are the predictions. \n",
    "\n",
    "**TL/DR: No I can't reproduce his sketches.  The graph I get for sum of the squared error (SE) doesn't have the wiggles that his drawing on the left does. *(Perhaps he was just doodling an example of an arbitrary non-convex function, rather than the squared loss in particular?)*   Takeways at the bottom of this, re. the difference between a convex *loss function (by itself)* vs. a convex loss for a *problem* -- i.e. the individual terms are convex for either function, but the *sum* of these terms is actually not strictly convex for either function (for this problem). **\n",
    "\n",
    "I read a few posts about this first...\n",
    "* [Math StackExchange: Show that logistic regression with squared loss function is non-convex](https://math.stackexchange.com/questions/1985008/show-that-logistic-regression-with-squared-loss-function-is-non-convex/2176466), which includes a link to [this nice demo on Desmos](https://www.desmos.com/calculator/kxz6lzszf9)\n",
    "* https://math.stackexchange.com/questions/2193478/loss-function-for-logistic-regression\n",
    "* https://en.wikipedia.org/wiki/Loss_functions_for_classification seems to say that squared loss is convex. ?? \n",
    "\n",
    "...but then wanted to try for myself.  As follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preliminary set up. ipympl allows for interactive plots; replace it with inline if not working\n",
    "#%matplotlib ipympl\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
